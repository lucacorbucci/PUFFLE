{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import dill\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nodes = [\n",
    "    \"15\",\n",
    "    \"71\",\n",
    "    \"0\",\n",
    "    \"87\",\n",
    "    \"19\",\n",
    "    \"120\",\n",
    "    \"85\",\n",
    "    \"127\",\n",
    "    \"32\",\n",
    "    \"149\",\n",
    "    \"38\",\n",
    "    \"23\",\n",
    "    \"126\",\n",
    "    \"72\",\n",
    "    \"7\",\n",
    "    \"66\",\n",
    "    \"2\",\n",
    "    \"31\",\n",
    "    \"128\",\n",
    "    \"130\",\n",
    "    \"89\",\n",
    "    \"115\",\n",
    "    \"131\",\n",
    "    \"79\",\n",
    "    \"36\",\n",
    "    \"77\",\n",
    "    \"81\",\n",
    "    \"117\",\n",
    "    \"140\",\n",
    "    \"57\",\n",
    "    \"53\",\n",
    "    \"30\",\n",
    "    \"139\",\n",
    "    \"21\",\n",
    "    \"138\",\n",
    "    \"141\",\n",
    "    \"142\",\n",
    "    \"50\",\n",
    "    \"78\",\n",
    "    \"116\",\n",
    "    \"12\",\n",
    "    \"83\",\n",
    "    \"56\",\n",
    "    \"64\",\n",
    "    \"24\",\n",
    "    \"61\",\n",
    "    \"92\",\n",
    "    \"84\",\n",
    "    \"34\",\n",
    "    \"6\",\n",
    "]\n",
    "\n",
    "counters_nodes = dill.load(open(\"./selected.pkl\", \"rb\"))\n",
    "test_nodes_columns_disparity_dataset = [\n",
    "    f\"Test Node {name} - Disp. Dataset\" for name in test_nodes\n",
    "]\n",
    "\n",
    "all_nodes = list(range(150))\n",
    "\n",
    "all_nodes_columns_disparity_dataset = [\n",
    "    f\"Test Node {name} - Disp. Dataset\" for name in all_nodes\n",
    "]\n",
    "\n",
    "\n",
    "test_nodes_columns_accuracy = [f\"Test Node {name} - Acc.\" for name in test_nodes]\n",
    "test_nodes_columns_disparity = [f\"Test Node {name} - Disp.\" for name in test_nodes]\n",
    "\n",
    "\n",
    "test_nodes_columns_test_accuracy = \"Test Disparity\"\n",
    "test_nodes_columns_test_disparity = \"Test Accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_nodes(df):\n",
    "    train_nodes_tmp = list(set([str(item) for item in range(0, 150)]) - set(test_nodes))\n",
    "\n",
    "    train_nodes = []\n",
    "    for node in train_nodes_tmp:\n",
    "        if f\"Disparity Client {node} After Local train\" in df.columns:\n",
    "            train_nodes.append(node)\n",
    "\n",
    "    train_nodes_columns_lambda = [\n",
    "        f\"Lambda Client {node_name}\" for node_name in train_nodes\n",
    "    ]\n",
    "    train_nodes_columns_disparity_dataset = [\n",
    "        f\"Disparity Dataset Client {name}\" for name in train_nodes\n",
    "    ]\n",
    "    train_nodes_columns_disparity = [\n",
    "        f\"Disparity Client {node_name} After Local train\" for node_name in train_nodes\n",
    "    ]\n",
    "    train_nodes_columns_disparity_before = [\n",
    "        f\"Disparity Client {node_name} Before local train\" for node_name in train_nodes\n",
    "    ]\n",
    "    return (\n",
    "        train_nodes,\n",
    "        train_nodes_columns_lambda,\n",
    "        train_nodes_columns_disparity_dataset,\n",
    "        train_nodes_columns_disparity,\n",
    "        train_nodes_columns_disparity_before,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters_nodes = dill.load(open(\"./not_selected.pkl\", \"rb\"))\n",
    "node_disparities_fair = {}\n",
    "node_disparities_unfair = {}\n",
    "\n",
    "for node_name, node_info in counters_nodes.items():\n",
    "    disparity = node_info[0]\n",
    "    if disparity < 0.1:\n",
    "        node_disparities_fair[node_name] = disparity\n",
    "    else:\n",
    "        node_disparities_unfair[node_name] = disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_nodes = dill.load(open('./not_selected.pkl', 'rb'))\n",
    "# node_disparities_fair = {}\n",
    "# node_disparities_unfair = {}\n",
    "\n",
    "# for node_name, node_info in train_nodes.items():\n",
    "#     disparity = node_info[0]\n",
    "#     if disparity < 0.1:\n",
    "#         node_disparities_fair[node_name] = disparity\n",
    "#     else:\n",
    "#         node_disparities_unfair[node_name] = disparity\n",
    "# print(len(node_disparities_fair))\n",
    "# print(len(node_disparities_unfair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(api_link):\n",
    "    api = wandb.Api()\n",
    "    run = api.run(api_link)\n",
    "    df = pd.DataFrame(run.scan_history())\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_nan(column_names, dataframe):\n",
    "    column_names = [\n",
    "        column_name for column_name in column_names if column_name in dataframe.columns\n",
    "    ]\n",
    "\n",
    "    current_df = dataframe[column_names]\n",
    "    # consider each column in training_data_disparity independently and\n",
    "    # remove the rows where we have NaN\n",
    "    new_columns = []\n",
    "\n",
    "    for column in current_df.columns:\n",
    "        new_values = list(current_df[column].dropna())\n",
    "        new_columns.append(new_values)\n",
    "\n",
    "    # if the lists have different lengths, we need to modify them so that\n",
    "    # we have the same length:\n",
    "    min_size = min([len(item) for item in new_columns])\n",
    "    new_columns = [item[:min_size] for item in new_columns]\n",
    "\n",
    "    # create the new dataframe with baseline_test_nodes_columns_disparity_dataset as columns\n",
    "    # names and new_columns as values\n",
    "    new_df = pd.DataFrame(dict(zip(column_names, new_columns)))\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_disparity_dataset(df, node_names, file_name, title):\n",
    "    nodes_with_disparity = {}\n",
    "    for node_name, column in zip(node_names, df.columns):\n",
    "        # get the last value of the column\n",
    "        nodes_with_disparity[node_name] = df[column].iloc[-1]\n",
    "\n",
    "    figure(figsize=(20, 8), dpi=80)\n",
    "    # sort the nodes by disparity\n",
    "    nodes_with_disparity = dict(\n",
    "        sorted(nodes_with_disparity.items(), key=lambda item: item[1])\n",
    "    )\n",
    "    plt.bar(nodes_with_disparity.keys(), nodes_with_disparity.values())\n",
    "    plt.xlabel(\"Nodes\")\n",
    "    plt.ylabel(\"Disparity\")\n",
    "    # font size\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "    # rotate xticks\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.title(title)\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "    return nodes_with_disparity.keys(), nodes_with_disparity.values()\n",
    "\n",
    "\n",
    "def plot_different_distributions(counters_nodes, sorted_nodes, file_name):\n",
    "    # we want to plot the different distributions of the nodes\n",
    "    # in particular the amount of samples of group (0,1) and (1,1)\n",
    "    one_zeros = []\n",
    "    one_ones = []\n",
    "    for node_name in sorted_nodes:\n",
    "        one_zero = counters_nodes[node_name][1][(0, 1)]\n",
    "        one_one = counters_nodes[node_name][1][(1, 1)]\n",
    "        one_zeros.append(one_zero)\n",
    "        one_ones.append(one_one)\n",
    "\n",
    "    # plot a grouped bar chart with the values of one_zeros and one_ones\n",
    "    # for each node. For each node I want to show two bars, one for one_zeros\n",
    "    # and one for one_ones\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    plt.bar(sorted_nodes, one_zeros, width=0.4, align=\"edge\")\n",
    "    plt.bar(sorted_nodes, one_ones, width=-0.4, align=\"edge\")\n",
    "    plt.legend([\"(0,1)\", \"(1,1)\"])\n",
    "    plt.xlabel(\"Nodes\")\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    # font size\n",
    "    plt.rcParams.update({\"font.size\": 17})\n",
    "    # rotate xticks\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Distribution of samples (1,1) and (0,1) in the test nodes\")\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "\n",
    "# def plot_fixed_vs_tunable(baseline, fixed, tunable, nodes, file_name, sorted_nodes, target):\n",
    "#     baseline_disparity_dict = {}\n",
    "#     fixed_disparity_dict = {}\n",
    "#     tunable_disparit_dict = {}\n",
    "#     for node_name, column in zip(nodes, baseline.columns):\n",
    "#         # get the last value of the column\n",
    "#         baseline_disparity_dict[node_name] = baseline[column].iloc[-1]\n",
    "#         fixed_disparity_dict[node_name] = fixed[column].iloc[-1]\n",
    "#         tunable_disparit_dict[node_name] = tunable[column].iloc[-1]\n",
    "\n",
    "#     baseline_disparity = []\n",
    "#     fixed_disparity = []\n",
    "#     tunable_disparity = []\n",
    "#     for node in sorted_nodes:\n",
    "#         baseline_disparity.append(baseline_disparity_dict[node])\n",
    "#         fixed_disparity.append(fixed_disparity_dict[node])\n",
    "#         tunable_disparity.append(tunable_disparit_dict[node])\n",
    "\n",
    "#     width = 0.27\n",
    "#     figure(figsize=(25, 8), dpi=80)\n",
    "#     indexes = np.arange(len(nodes))\n",
    "#     plt.bar(indexes, baseline_disparity, width=width)\n",
    "#     plt.bar(indexes+width, fixed_disparity, width=width)\n",
    "#     plt.bar(indexes+width*2, tunable_disparity, width=width)\n",
    "#     plt.axhline(y=target, color='purple', linestyle='--')\n",
    "#     plt.legend([\"Baseline\", \"Fixed\", \"Tunable\"])\n",
    "#     plt.xticks(indexes + width, nodes)\n",
    "#     plt.rcParams.update({'font.size': 20})\n",
    "#     plt.xlabel(\"Nodes\")\n",
    "#     plt.ylabel(\"Disparity\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(\"Comparison between the test disparity of the baseline, fixed and tunable Lambda\")\n",
    "#     plt.savefig(file_name)\n",
    "\n",
    "\n",
    "def plot_fixed_vs_tunable_line(\n",
    "    disparity_dataset,\n",
    "    baseline,\n",
    "    fixed,\n",
    "    tunable,\n",
    "    nodes,\n",
    "    file_name,\n",
    "    sorted_nodes,\n",
    "    target,\n",
    "    title,\n",
    "):\n",
    "    # Plots the final disparity of the baseline, fixed and tunable models for each node\n",
    "    baseline_disparity_dict = {}\n",
    "    fixed_disparity_dict = {}\n",
    "    tunable_disparit_dict = {}\n",
    "\n",
    "    for node_name, column in zip(nodes, baseline.columns):\n",
    "        # get the last value of the column\n",
    "        if column in baseline.columns:\n",
    "            baseline_disparity_dict[node_name] = baseline[column].iloc[-1]\n",
    "        # check if the dataframe fixed and tunable are None or not\n",
    "        # and if the column is in the dataframe\n",
    "        if column in fixed.columns:\n",
    "            fixed_disparity_dict[node_name] = fixed[column].iloc[-1]\n",
    "        if column in tunable.columns:\n",
    "            tunable_disparit_dict[node_name] = tunable[column].iloc[-1]\n",
    "\n",
    "    baseline_disparity = []\n",
    "    fixed_disparity = []\n",
    "    tunable_disparity = []\n",
    "    for node in sorted_nodes:\n",
    "        if node in baseline_disparity_dict:\n",
    "            baseline_disparity.append(baseline_disparity_dict[node])\n",
    "        if fixed_disparity_dict != {} and node in fixed_disparity_dict:\n",
    "            fixed_disparity.append(fixed_disparity_dict[node])\n",
    "        if tunable_disparit_dict != {} and node in tunable_disparit_dict:\n",
    "            tunable_disparity.append(tunable_disparit_dict[node])\n",
    "\n",
    "    # get the training disparity\n",
    "    nodes_with_disparity = {}\n",
    "    for node_name, column in zip(nodes, disparity_dataset.columns):\n",
    "        # get the last value of the column\n",
    "        nodes_with_disparity[node_name] = disparity_dataset[column].iloc[-1]\n",
    "\n",
    "    training_disparity = [nodes_with_disparity[node_name] for node_name in sorted_nodes]\n",
    "\n",
    "    width = 0.27\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    indexes = np.arange(len(sorted_nodes))\n",
    "\n",
    "    plt.scatter(\n",
    "        indexes, baseline_disparity, marker=\"x\", linewidths=4, s=100, color=\"red\"\n",
    "    )\n",
    "    if fixed_disparity:\n",
    "        plt.scatter(\n",
    "            indexes, fixed_disparity, marker=\"o\", linewidths=4, s=100, color=\"green\"\n",
    "        )\n",
    "    if tunable_disparity:\n",
    "        plt.scatter(\n",
    "            indexes, tunable_disparity, marker=\"+\", linewidths=4, s=100, color=\"blue\"\n",
    "        )\n",
    "    plt.scatter(\n",
    "        indexes, training_disparity, marker=\"*\", linewidths=4, s=100, color=\"black\"\n",
    "    )\n",
    "\n",
    "    plt.plot(indexes, baseline_disparity, linewidth=1, color=\"red\")\n",
    "    if fixed_disparity:\n",
    "        plt.plot(indexes, fixed_disparity, linewidth=1, color=\"green\")\n",
    "    if tunable_disparity:\n",
    "        plt.plot(indexes, tunable_disparity, linewidth=1, color=\"blue\")\n",
    "    plt.plot(indexes, training_disparity, linewidth=1, color=\"black\")\n",
    "\n",
    "    plt.axhline(y=target, color=\"purple\", linestyle=\"--\")\n",
    "\n",
    "    if tunable_disparity and fixed_disparity:\n",
    "        plt.legend([\"Baseline\", \"Fixed\", \"Tunable\", \"Dataset Disparity\"])\n",
    "    else:\n",
    "        plt.legend([\"Baseline\", \"Dataset Disparity\"])\n",
    "    plt.xticks(indexes, sorted_nodes)\n",
    "    plt.rcParams.update({\"font.size\": 16})\n",
    "    plt.xlabel(\"Nodes\")\n",
    "    plt.ylabel(\"Disparity\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "\n",
    "# def plot_difference_disparity(baseline, fixed, tunable, nodes, file_name, sorted_nodes, target):\n",
    "#     # plot the difference between the disparity of the baseline and the disparity of the fixed\n",
    "#     # and tunable models. This is a grouped bar chart with 2 bars for each node, one for the fixed\n",
    "#     # and one for the tunable.\n",
    "#     baseline_disparity_dict = {}\n",
    "#     fixed_disparity_dict = {}\n",
    "#     tunable_disparit_dict = {}\n",
    "#     for node_name, column in zip(nodes, baseline.columns):\n",
    "#         # get the last value of the column\n",
    "#         baseline_disparity_dict[node_name] = baseline[column].iloc[-1]\n",
    "#         fixed_disparity_dict[node_name] = fixed[column].iloc[-1]\n",
    "#         tunable_disparit_dict[node_name] = tunable[column].iloc[-1]\n",
    "\n",
    "#     fixed_disparity = []\n",
    "#     tunable_disparity = []\n",
    "#     for node in sorted_nodes:\n",
    "#         fixed_disparity.append(baseline_disparity_dict[node]-fixed_disparity_dict[node])\n",
    "#         tunable_disparity.append(baseline_disparity_dict[node]-tunable_disparit_dict[node])\n",
    "\n",
    "#     width = 0.27\n",
    "#     figure(figsize=(25, 8), dpi=80)\n",
    "#     indexes = np.arange(len(nodes))\n",
    "#     plt.bar(indexes, fixed_disparity, width=width)\n",
    "#     plt.bar(indexes+width, tunable_disparity, width=width)\n",
    "#     plt.legend([\"Fixed\", \"Tunable\"])\n",
    "#     plt.xticks(indexes + width, nodes)\n",
    "#     plt.rcParams.update({'font.size': 20})\n",
    "#     plt.xlabel(\"Nodes\")\n",
    "#     plt.ylabel(\"Difference\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(\"Difference between the test disparity of the baseline and the disparity of the fixed and tunable Lambda\")\n",
    "#     plt.savefig(file_name)\n",
    "\n",
    "\n",
    "def plot_difference_disparity_line(\n",
    "    baseline, fixed, tunable, nodes, file_name, sorted_nodes, target\n",
    "):\n",
    "    # plot the difference between the disparity of the baseline and the disparity of the fixed\n",
    "    # and tunable models. This is a grouped bar chart with 2 bars for each node, one for the fixed\n",
    "    # and one for the tunable.\n",
    "    baseline_disparity_dict = {}\n",
    "    fixed_disparity_dict = {}\n",
    "    tunable_disparit_dict = {}\n",
    "    for node_name, column in zip(nodes, baseline.columns):\n",
    "        # get the last value of the column\n",
    "        baseline_disparity_dict[node_name] = baseline[column].iloc[-1]\n",
    "        fixed_disparity_dict[node_name] = fixed[column].iloc[-1]\n",
    "        tunable_disparit_dict[node_name] = tunable[column].iloc[-1]\n",
    "\n",
    "    fixed_disparity = []\n",
    "    tunable_disparity = []\n",
    "    for node in sorted_nodes:\n",
    "        fixed_disparity.append(\n",
    "            baseline_disparity_dict[node] - fixed_disparity_dict[node]\n",
    "        )\n",
    "        tunable_disparity.append(\n",
    "            baseline_disparity_dict[node] - tunable_disparit_dict[node]\n",
    "        )\n",
    "\n",
    "    width = 0.27\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    indexes = np.arange(len(sorted_nodes))\n",
    "\n",
    "    plt.scatter(\n",
    "        indexes, fixed_disparity, marker=\"o\", linewidths=4, s=100, color=\"green\"\n",
    "    )\n",
    "    plt.scatter(\n",
    "        indexes, tunable_disparity, marker=\"+\", linewidths=4, s=100, color=\"blue\"\n",
    "    )\n",
    "    # ax = plt.axes()\n",
    "    # ax.yaxis.grid()\n",
    "\n",
    "    plt.plot(indexes, fixed_disparity, linewidth=1, color=\"green\")\n",
    "    plt.plot(indexes, tunable_disparity, linewidth=1, color=\"blue\")\n",
    "    plt.legend([\"Fixed\", \"Tunable\"])\n",
    "    plt.xticks(indexes, sorted_nodes)\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "    plt.xlabel(\"Nodes\")\n",
    "    plt.ylabel(\"Difference\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\n",
    "        \"Difference between the test disparity of the baseline and the disparity of the fixed and tunable Lambda (At LAST FL ROUND)\"\n",
    "    )\n",
    "    plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_fixed_vs_tunable_accuracy(baseline, fixed, tunable, nodes, file_name, sorted_nodes, target):\n",
    "#     # Plots the final accuracy of the baseline, fixed and tunable models for each node\n",
    "#     baseline_disparity_dict = {}\n",
    "#     fixed_disparity_dict = {}\n",
    "#     tunable_disparit_dict = {}\n",
    "#     for node_name, column in zip(nodes, baseline.columns):\n",
    "#         # get the last value of the column\n",
    "#         baseline_disparity_dict[node_name] = baseline[column].iloc[-1]\n",
    "#         fixed_disparity_dict[node_name] = fixed[column].iloc[-1]\n",
    "#         tunable_disparit_dict[node_name] = tunable[column].iloc[-1]\n",
    "\n",
    "#     baseline_disparity = []\n",
    "#     fixed_disparity = []\n",
    "#     tunable_disparity = []\n",
    "#     for node in sorted_nodes:\n",
    "#         baseline_disparity.append(baseline_disparity_dict[node])\n",
    "#         fixed_disparity.append(fixed_disparity_dict[node])\n",
    "#         tunable_disparity.append(tunable_disparit_dict[node])\n",
    "\n",
    "#     width = 0.27\n",
    "#     figure(figsize=(25, 8), dpi=80)\n",
    "#     indexes = np.arange(len(nodes))\n",
    "#     plt.bar(indexes, baseline_disparity, width=width)\n",
    "#     plt.bar(indexes+width, fixed_disparity, width=width)\n",
    "#     plt.bar(indexes+width*2, tunable_disparity, width=width)\n",
    "#     # plt.axhline(y=target, color='purple', linestyle='--')\n",
    "#     plt.legend([\"Baseline\", \"Fixed\", \"Tunable\"])\n",
    "#     plt.xticks(indexes + width, nodes)\n",
    "#     plt.rcParams.update({'font.size': 20})\n",
    "#     plt.xlabel(\"Nodes\")\n",
    "#     plt.ylabel(\"Accuracy\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(\"Comparison between the test accuracy of the baseline, fixed and tunable Lambda\")\n",
    "#     plt.savefig(file_name)\n",
    "\n",
    "\n",
    "def plot_fixed_vs_tunable_accuracy_line(\n",
    "    baseline, fixed, tunable, nodes, file_name, sorted_nodes, target\n",
    "):\n",
    "    # Plots the final accuracy of the baseline, fixed and tunable models for each node\n",
    "    baseline_disparity_dict = {}\n",
    "    fixed_disparity_dict = {}\n",
    "    tunable_disparit_dict = {}\n",
    "    for node_name, column in zip(nodes, baseline.columns):\n",
    "        # get the last value of the column\n",
    "        baseline_disparity_dict[node_name] = baseline[column].iloc[-1]\n",
    "        fixed_disparity_dict[node_name] = fixed[column].iloc[-1]\n",
    "        tunable_disparit_dict[node_name] = tunable[column].iloc[-1]\n",
    "\n",
    "    baseline_disparity = []\n",
    "    fixed_disparity = []\n",
    "    tunable_disparity = []\n",
    "    for node in sorted_nodes:\n",
    "        baseline_disparity.append(baseline_disparity_dict[node])\n",
    "        fixed_disparity.append(fixed_disparity_dict[node])\n",
    "        tunable_disparity.append(tunable_disparit_dict[node])\n",
    "\n",
    "    width = 0.27\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    indexes = np.arange(len(nodes))\n",
    "\n",
    "    # plt.axhline(y=target, color='purple', linestyle='--')\n",
    "\n",
    "    plt.scatter(\n",
    "        indexes, baseline_disparity, marker=\"x\", linewidths=3, s=100, color=\"red\"\n",
    "    )\n",
    "    plt.scatter(\n",
    "        indexes, fixed_disparity, marker=\"o\", linewidths=3, s=100, color=\"green\"\n",
    "    )\n",
    "    plt.scatter(\n",
    "        indexes, tunable_disparity, marker=\"+\", linewidths=3, s=100, color=\"blue\"\n",
    "    )\n",
    "\n",
    "    plt.plot(indexes, baseline_disparity, linewidth=1, color=\"red\")\n",
    "    plt.plot(indexes, fixed_disparity, linewidth=1, color=\"green\")\n",
    "    plt.plot(indexes, tunable_disparity, linewidth=1, color=\"blue\")\n",
    "    # ax = plt.axes()\n",
    "    # ax.yaxis.grid()\n",
    "\n",
    "    plt.legend([\"Baseline\", \"Fixed\", \"Tunable\"])\n",
    "    plt.xticks(indexes, sorted_nodes)\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "    plt.xlabel(\"Nodes\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\n",
    "        \"Comparison between the test accuracy of the baseline, fixed and tunable Lambda (LAST FL ROUND)\"\n",
    "    )\n",
    "    plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lambda(baseline, fixed, tunable, nodes, file_name, target):\n",
    "    accuracy_baseline = list(baseline[\"Aggregated Lambda\"].dropna())\n",
    "    accuracy_fixed = list(fixed[\"Aggregated Lambda\"].dropna())\n",
    "    accuracy_tunable = list(tunable[\"Aggregated Lambda\"].dropna())\n",
    "\n",
    "    # plot the three lines\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    # grid\n",
    "    ax = plt.axes()\n",
    "    ax.yaxis.grid()\n",
    "    plt.plot(accuracy_baseline, linewidth=2, color=\"red\")\n",
    "    plt.plot(accuracy_fixed, linewidth=2, color=\"green\")\n",
    "    plt.plot(accuracy_tunable, linewidth=2, color=\"blue\")\n",
    "    plt.legend([\"Baseline\", \"Fixed\", \"Tunable\"])\n",
    "    plt.xlabel(\"FL Rounds\")\n",
    "    plt.ylabel(\"Lambda\")\n",
    "    plt.title(f\"Comparison of the Lambda in the Baseline, Fixed and Tunable models\")\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "\n",
    "def plot_accuracy_comparison(baseline, fixed, tunable, nodes, file_name, target, test, title):\n",
    "    if test:\n",
    "        accuracy_baseline = list(baseline[\"Test Accuracy\"].dropna())\n",
    "        accuracy_fixed = list(fixed[\"Test Accuracy\"].dropna())\n",
    "        accuracy_tunable = list(tunable[\"Test Accuracy\"].dropna())\n",
    "    else:\n",
    "        accuracy_baseline = list(baseline[\"Train Accuracy\"].dropna())\n",
    "        accuracy_fixed = list(fixed[\"Train Accuracy\"].dropna())\n",
    "        accuracy_tunable = list(tunable[\"Train Accuracy\"].dropna())\n",
    "    # plot the three lines\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    # grid\n",
    "    ax = plt.axes()\n",
    "    ax.yaxis.grid()\n",
    "    plt.plot(accuracy_baseline, linewidth=2, color=\"red\")\n",
    "    plt.plot(accuracy_fixed, linewidth=2, color=\"green\")\n",
    "    plt.plot(accuracy_tunable, linewidth=2, color=\"blue\")\n",
    "    plt.legend([\"Baseline\", \"Fixed\", \"Tunable\"])\n",
    "    plt.xlabel(\"FL Rounds\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\n",
    "        title\n",
    "    )\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "\n",
    "def plot_disparity_comparison(baseline, fixed, tunable, nodes, file_name, target, test):\n",
    "    if test:\n",
    "        disparity_baseline = list(baseline[\"Test Disparity with statistics\"].dropna())\n",
    "        disparity_fixed = list(fixed[\"Test Disparity with statistics\"].dropna())\n",
    "        disparity_tunable = list(tunable[\"Test Disparity with statistics\"].dropna())\n",
    "    else:\n",
    "        disparity_baseline = list(\n",
    "            baseline[\"Training Disparity with statistics\"].dropna()\n",
    "        )\n",
    "        disparity_fixed = list(fixed[\"Training Disparity with statistics\"].dropna())\n",
    "        disparity_tunable = list(tunable[\"Training Disparity with statistics\"].dropna())\n",
    "    # plot the three lines\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    ax = plt.axes()\n",
    "    ax.yaxis.grid()\n",
    "    plt.plot(disparity_baseline, linewidth=2, color=\"red\")\n",
    "    plt.plot(disparity_fixed, linewidth=2, color=\"green\")\n",
    "    plt.plot(disparity_tunable, linewidth=2, color=\"blue\")\n",
    "    # plot horizontal line at target\n",
    "    plt.axhline(y=target, color=\"purple\", linestyle=\"--\")\n",
    "\n",
    "    plt.legend([\"Baseline\", \"Fixed\", \"Tunable\"])\n",
    "    plt.xlabel(\"FL Rounds\")\n",
    "    plt.ylabel(\"Disparity\")\n",
    "    plt.title(\n",
    "        \"Test Disparity comparison between the baseline, fixed and tunable Lambda\"\n",
    "        if test\n",
    "        else \"Training Disparity comparison between the baseline, fixed and tunable Lambda\"\n",
    "    )\n",
    "    plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lambda_with_std(fixed, tunable, file_name, train_nodes_columns_lambda):\n",
    "    \"\"\"\n",
    "    This function computes the mean and the standard deviation of the lambda\n",
    "    values of the experiment with tunable Lambda. Before doing this, we have to\n",
    "    remove the NaN values from the dataframe and to align the values of the\n",
    "    steps with the values of the fl rounds\n",
    "    \"\"\"\n",
    "    lambdas = [\n",
    "        list(tunable[column].values)\n",
    "        for column in tunable[train_nodes_columns_lambda].columns\n",
    "    ]\n",
    "    print(len(lambdas[0]))\n",
    "    # check if it is nan\n",
    "    print(np.isnan(lambdas[0][0]))\n",
    "    # get the maximum length of the lambdas list then for each list I want to\n",
    "    # check element and if I find a value that is not nan I want to append to a list\n",
    "    tmp_list = []\n",
    "    fl_round_list = []\n",
    "    for i in range(len(lambdas[0])):\n",
    "        for lambda_list in lambdas:\n",
    "            if not np.isnan(lambda_list[i]):\n",
    "                if len(tmp_list) < 20:\n",
    "                    tmp_list.append(lambda_list[i])\n",
    "                else:\n",
    "                    fl_round_list.append(copy.deepcopy(tmp_list))\n",
    "                    tmp_list = []\n",
    "\n",
    "    # mean of the fl_round_list\n",
    "    mean_fl_round_list = [np.mean(fl_round) for fl_round in fl_round_list]\n",
    "    # std of the fl_round_list\n",
    "    std_fl_round_list = [np.std(fl_round) for fl_round in fl_round_list]\n",
    "\n",
    "    # plot the mean and the standard deviation with error bars\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    ax = plt.axes()\n",
    "    ax.yaxis.grid()\n",
    "    plt.errorbar(\n",
    "        range(len(mean_fl_round_list)),\n",
    "        mean_fl_round_list,\n",
    "        yerr=std_fl_round_list,\n",
    "        fmt=\"o\",\n",
    "        color=\"blue\",\n",
    "        ecolor=\"lightblue\",\n",
    "        elinewidth=3,\n",
    "        capsize=0,\n",
    "    )\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "    lambda_fixed = list(fixed[\"Aggregated Lambda\"].dropna())\n",
    "    plt.plot(lambda_fixed, linewidth=2, color=\"green\")\n",
    "    plt.plot(mean_fl_round_list, linewidth=2, color=\"blue\")\n",
    "\n",
    "    plt.legend([\"Fixed\", \"Tunable\"])\n",
    "    plt.xlabel(\"FL Rounds\")\n",
    "    plt.ylabel(\"Lambda\")\n",
    "    plt.title(\"Fixed Lambda vs Avg. Tunable Lambda with std\")\n",
    "    plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_nodes_disparity_dataset(all_values, all_nodes, experiment):\n",
    "    dictionary = dict(zip(all_nodes, all_values))\n",
    "\n",
    "    # bar plot with all the nodes\n",
    "    figure(figsize=(25, 8), dpi=80)\n",
    "    # sort the nodes by disparity\n",
    "    dictionary = dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "    plt.bar(dictionary.keys(), dictionary.values())\n",
    "    plt.xlabel(\"Nodes\")\n",
    "    plt.ylabel(\"Disparity\")\n",
    "    # font size\n",
    "    plt.rcParams.update({\"font.size\": 20})\n",
    "    # font size of x axis ticks\n",
    "    plt.xticks(fontsize=10)\n",
    "    # remove xticks\n",
    "    plt.xticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Disparity Dataset for all nodes\")\n",
    "    plt.savefig(f\"all_nodes_disparity_dataset_{experiment}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot_baseline(\n",
    "    baseline_link,\n",
    "    experiment,\n",
    "    target,\n",
    "):\n",
    "    df_baseline = get_df(baseline_link)\n",
    "\n",
    "    all_nodes_disparity_dataset = remove_nan(\n",
    "        all_nodes_columns_disparity_dataset, df_baseline\n",
    "    )\n",
    "\n",
    "    baseline_test_nodes_columns_disparity_dataset = remove_nan(\n",
    "        test_nodes_columns_disparity_dataset, df_baseline\n",
    "    )\n",
    "    baseline_test_nodes_columns_accuracy = remove_nan(\n",
    "        test_nodes_columns_accuracy, df_baseline\n",
    "    )\n",
    "    baseline_test_nodes_columns_disparity = remove_nan(\n",
    "        test_nodes_columns_disparity, df_baseline\n",
    "    )\n",
    "\n",
    "    (\n",
    "        train_nodes,\n",
    "        train_nodes_columns_lambda,\n",
    "        train_nodes_columns_disparity_dataset,\n",
    "        train_nodes_columns_disparity,\n",
    "        train_nodes_columns_disparity_before,\n",
    "    ) = get_train_nodes(df_baseline)\n",
    "    print(len(train_nodes))\n",
    "\n",
    "    # Be sure that we do not consider the validation nodes in the train nodes\n",
    "\n",
    "    current_columns = []\n",
    "    for column_name in train_nodes_columns_disparity_dataset:\n",
    "        if column_name in df_baseline.columns:\n",
    "            current_columns.append(column_name)\n",
    "\n",
    "    baseline_train_nodes_columns_disparity_dataset = remove_nan(\n",
    "        current_columns, df_baseline\n",
    "    )\n",
    "\n",
    "    current_columns = []\n",
    "    for column_name in train_nodes_columns_disparity:\n",
    "        current_columns.append(column_name)\n",
    "\n",
    "    baseline_train_nodes_columns_disparity = remove_nan(current_columns, df_baseline)\n",
    "\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"------------ Test dataset ------------\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    # print(\"++++++++++++ Disparity Dataset ++++++++++++\")\n",
    "    sorted_nodes_test, sorted_values_test = plot_disparity_dataset(\n",
    "        baseline_test_nodes_columns_disparity_dataset,\n",
    "        test_nodes,\n",
    "        file_name=f\"test_disparity_dataset_{experiment}.png\",\n",
    "        title=\"Test Dataset disparity for each test node\",\n",
    "    )\n",
    "\n",
    "    print(\"++++++++++++ Disparity Fixed vs Tunable ++++++++++++\")\n",
    "    plot_fixed_vs_tunable_line(\n",
    "        disparity_dataset=baseline_test_nodes_columns_disparity_dataset,\n",
    "        baseline=baseline_test_nodes_columns_disparity,\n",
    "        fixed=None,\n",
    "        tunable=None,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"test_comparison_disparities_{experiment}.png\",\n",
    "        sorted_nodes=sorted_nodes_test,\n",
    "        target=target,\n",
    "        title=\"Comparison between the test disparity of the baseline, fixed and tunable Lambda\",\n",
    "    )\n",
    "\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"------------ Train dataset ------------\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    print(\"++++++++++++ Disparity Dataset ++++++++++++\")\n",
    "    # Train dataset\n",
    "    sorted_nodes_train, sorted_values_train = plot_disparity_dataset(\n",
    "        baseline_train_nodes_columns_disparity_dataset,\n",
    "        train_nodes,\n",
    "        file_name=f\"train_disparity_dataset_{experiment}.png\",\n",
    "        title=\"Train Dataset disparity for each train node\",\n",
    "    )\n",
    "    print(\"++++++++++++ Disparity Fixed vs Tunable ++++++++++++\")\n",
    "    all_nodes = list(sorted_nodes_test) + list(sorted_nodes_train)\n",
    "    all_values = list(sorted_values_test) + list(sorted_values_train)\n",
    "    print(\n",
    "        f\"AVG DISPARITY {np.mean(all_values)}, STD {np.std(all_values)}, MEDIAN {np.median(all_values)}\"\n",
    "    )\n",
    "    print(len(all_nodes))\n",
    "    print(len(all_values))\n",
    "\n",
    "    sorted_nodes = plot_all_nodes_disparity_dataset(\n",
    "        all_values=all_values,\n",
    "        all_nodes=all_nodes,\n",
    "        experiment=experiment,\n",
    "    )\n",
    "\n",
    "    plot_fixed_vs_tunable_line(\n",
    "        disparity_dataset=baseline_train_nodes_columns_disparity_dataset,\n",
    "        baseline=baseline_train_nodes_columns_disparity,\n",
    "        fixed=None,\n",
    "        tunable=None,\n",
    "        nodes=train_nodes,\n",
    "        file_name=f\"train_fixed_vs_tunable_{experiment}.png\",\n",
    "        sorted_nodes=sorted_nodes_train,\n",
    "        target=target,\n",
    "        title=\"Comparison between the Train disparity of the baseline, fixed and tunable Lambda (LAST FL ROUND)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_plot(\n",
    "    baseline_link,\n",
    "    fixed_link,\n",
    "    tunable_link,\n",
    "    experiment,\n",
    "    target,\n",
    "    df_baseline=None,\n",
    "    df_fixed=None,\n",
    "    df_tunable=None,\n",
    "):\n",
    "    if df_baseline is None:\n",
    "        df_baseline = get_df(baseline_link)\n",
    "\n",
    "    if df_fixed is None:\n",
    "        df_fixed = get_df(fixed_link)\n",
    "\n",
    "    if df_tunable is None:\n",
    "        df_tunable = get_df(tunable_link)\n",
    "\n",
    "    (\n",
    "        train_nodes,\n",
    "        train_nodes_columns_lambda,\n",
    "        train_nodes_columns_disparity_dataset,\n",
    "        train_nodes_columns_disparity,\n",
    "        train_nodes_columns_disparity_before,\n",
    "    ) = get_train_nodes(df_fixed)\n",
    "    print(len(train_nodes))\n",
    "\n",
    "    all_nodes_disparity_dataset = remove_nan(\n",
    "        all_nodes_columns_disparity_dataset, df_baseline\n",
    "    )\n",
    "\n",
    "    baseline_test_nodes_columns_disparity_dataset = remove_nan(\n",
    "        test_nodes_columns_disparity_dataset, df_baseline\n",
    "    )\n",
    "    baseline_test_nodes_columns_accuracy = remove_nan(\n",
    "        test_nodes_columns_accuracy, df_baseline\n",
    "    )\n",
    "    baseline_test_nodes_columns_disparity = remove_nan(\n",
    "        test_nodes_columns_disparity, df_baseline\n",
    "    )\n",
    "\n",
    "    fixed_test_nodes_columns_disparity_dataset = remove_nan(\n",
    "        test_nodes_columns_disparity_dataset, df_fixed\n",
    "    )\n",
    "    fixed_test_nodes_columns_accuracy = remove_nan(\n",
    "        test_nodes_columns_accuracy, df_fixed\n",
    "    )\n",
    "    fixed_test_nodes_columns_disparity = remove_nan(\n",
    "        test_nodes_columns_disparity, df_fixed\n",
    "    )\n",
    "\n",
    "    tunable_test_nodes_columns_disparity_dataset = remove_nan(\n",
    "        test_nodes_columns_disparity_dataset, df_tunable\n",
    "    )\n",
    "    tunable_test_nodes_columns_accuracy = remove_nan(\n",
    "        test_nodes_columns_accuracy, df_tunable\n",
    "    )\n",
    "    tunable_test_nodes_columns_disparity = remove_nan(\n",
    "        test_nodes_columns_disparity, df_tunable\n",
    "    )\n",
    "\n",
    "    # Be sure that we do not consider the validation nodes in the train nodes\n",
    "\n",
    "    current_columns = []\n",
    "    for column_name in train_nodes_columns_disparity_dataset:\n",
    "        if column_name in df_baseline.columns:\n",
    "            current_columns.append(column_name)\n",
    "\n",
    "    baseline_train_nodes_columns_disparity_dataset = remove_nan(\n",
    "        current_columns, df_baseline\n",
    "    )\n",
    "\n",
    "    current_columns = []\n",
    "    for column_name in train_nodes_columns_disparity:\n",
    "        current_columns.append(column_name)\n",
    "\n",
    "    tunable_train_nodes_columns_disparity = remove_nan(current_columns, df_tunable)\n",
    "\n",
    "    fixed_train_nodes_columns_disparity = remove_nan(current_columns, df_fixed)\n",
    "\n",
    "    baseline_train_nodes_columns_disparity = remove_nan(current_columns, df_baseline)\n",
    "\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"------------ Test dataset ------------\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    # print(\"++++++++++++ Disparity All Nodes ++++++++++++\")\n",
    "    # _, _ = plot_disparity_dataset(\n",
    "    #     all_nodes_disparity_dataset,\n",
    "    #     all_nodes,\n",
    "    #     file_name=f\"all_nodes_disparity_dataset_{experiment}.png\",\n",
    "    #     title=\"Test Dataset disparity for each test node\",\n",
    "    # )\n",
    "\n",
    "    # print(\"++++++++++++ Disparity Dataset ++++++++++++\")\n",
    "    sorted_nodes_test, sorted_values_test = plot_disparity_dataset(\n",
    "        baseline_test_nodes_columns_disparity_dataset,\n",
    "        test_nodes,\n",
    "        file_name=f\"test_disparity_dataset_{experiment}.png\",\n",
    "        title=\"Test Dataset disparity for each test node\",\n",
    "    )\n",
    "\n",
    "    print(\"++++++++++++ Accuracy ++++++++++++\")\n",
    "    plot_accuracy_comparison(\n",
    "        baseline=df_baseline,\n",
    "        fixed=df_fixed,\n",
    "        tunable=df_tunable,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"test_accuracy_{experiment}.png\",\n",
    "        target=target,\n",
    "        test=True,\n",
    "        title=f\"Test Accuracy comparison between the baseline, fixed and tunable Lambda\"\n",
    "    )\n",
    "    print(\"++++++++++++ Disparity ++++++++++++\")\n",
    "    plot_disparity_comparison(\n",
    "        baseline=df_baseline,\n",
    "        fixed=df_fixed,\n",
    "        tunable=df_tunable,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"test_disparity{experiment}.png\",\n",
    "        target=target,\n",
    "        test=True,\n",
    "    )\n",
    "\n",
    "    print(\"++++++++++++ Disparity Fixed vs Tunable ++++++++++++\")\n",
    "    plot_fixed_vs_tunable_line(\n",
    "        disparity_dataset=baseline_test_nodes_columns_disparity_dataset,\n",
    "        baseline=baseline_test_nodes_columns_disparity,\n",
    "        fixed=fixed_test_nodes_columns_disparity,\n",
    "        tunable=tunable_test_nodes_columns_disparity,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"test_comparison_disparities_{experiment}.png\",\n",
    "        sorted_nodes=sorted_nodes_test,\n",
    "        target=target,\n",
    "        title=\"Comparison between the test disparity of the baseline, fixed and tunable Lambda\",\n",
    "    )\n",
    "\n",
    "    print(\"++++++++++++ Accuracy Fixed vs Tunable ++++++++++++\")\n",
    "    # plot_fixed_vs_tunable_accuracy(baseline=baseline_test_nodes_columns_accuracy, fixed=fixed_test_nodes_columns_accuracy, tunable=tunable_test_nodes_columns_accuracy, nodes=test_nodes, file_name=f\"comparison_accuracy_{experiment}.png\", sorted_nodes=sorted_nodes, target=target)\n",
    "    plot_fixed_vs_tunable_accuracy_line(\n",
    "        baseline=baseline_test_nodes_columns_accuracy,\n",
    "        fixed=fixed_test_nodes_columns_accuracy,\n",
    "        tunable=tunable_test_nodes_columns_accuracy,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"test_comparison_accuracy_{experiment}.png\",\n",
    "        sorted_nodes=sorted_nodes_test,\n",
    "        target=target,\n",
    "    )\n",
    "\n",
    "    print(\"++++++++++++ Difference disparity Fixed vs Tunable ++++++++++++\")\n",
    "    # plot_difference_disparity(baseline=baseline_test_nodes_columns_disparity, fixed=fixed_test_nodes_columns_disparity, tunable=tunable_test_nodes_columns_disparity, nodes=test_nodes, file_name=f\"difference_disparity_{experiment}.png\", sorted_nodes=sorted_nodes, target=target)\n",
    "    plot_difference_disparity_line(\n",
    "        baseline=baseline_test_nodes_columns_disparity,\n",
    "        fixed=fixed_test_nodes_columns_disparity,\n",
    "        tunable=tunable_test_nodes_columns_disparity,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"test_difference_disparity_{experiment}.png\",\n",
    "        sorted_nodes=sorted_nodes_test,\n",
    "        target=target,\n",
    "    )\n",
    "\n",
    "    print(\"--------------------------------------\")\n",
    "    print(\"------------ Train dataset ------------\")\n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "    print(\"++++++++++++ Disparity Dataset ++++++++++++\")\n",
    "    # Train dataset\n",
    "    sorted_nodes_train, sorted_values_train = plot_disparity_dataset(\n",
    "        baseline_train_nodes_columns_disparity_dataset,\n",
    "        train_nodes,\n",
    "        file_name=f\"train_disparity_dataset_{experiment}.png\",\n",
    "        title=\"Train Dataset disparity for each train node\",\n",
    "    )\n",
    "    print(\"++++++++++++ Disparity Fixed vs Tunable ++++++++++++\")\n",
    "    all_nodes = list(sorted_nodes_test) + list(sorted_nodes_train)\n",
    "    all_values = list(sorted_values_test) + list(sorted_values_train)\n",
    "    print(\n",
    "        f\"AVG DISPARITY {np.mean(all_values)}, STD {np.std(all_values)}, MEDIAN {np.median(all_values)}\"\n",
    "    )\n",
    "    print(len(all_nodes))\n",
    "    print(len(all_values))\n",
    "\n",
    "    sorted_nodes = plot_all_nodes_disparity_dataset(\n",
    "        all_values=all_values,\n",
    "        all_nodes=all_nodes,\n",
    "        experiment=experiment,\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"++++++++++++ Accuracy ++++++++++++\")\n",
    "    plot_accuracy_comparison(\n",
    "        baseline=df_baseline,\n",
    "        fixed=df_fixed,\n",
    "        tunable=df_tunable,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"train_accuracy_{experiment}.png\",\n",
    "        target=target,\n",
    "        test=False, \n",
    "        title=f\"Train Accuracy comparison between the baseline, fixed and tunable Lambda\"\n",
    "    )\n",
    "    plot_disparity_comparison(\n",
    "        baseline=df_baseline,\n",
    "        fixed=df_fixed,\n",
    "        tunable=df_tunable,\n",
    "        nodes=test_nodes,\n",
    "        file_name=f\"train_disparity_comparison_{experiment}.png\",\n",
    "        target=target,\n",
    "        test=False,\n",
    "    )\n",
    "\n",
    "    plot_fixed_vs_tunable_line(\n",
    "        disparity_dataset=baseline_train_nodes_columns_disparity_dataset,\n",
    "        baseline=baseline_train_nodes_columns_disparity,\n",
    "        fixed=fixed_train_nodes_columns_disparity,\n",
    "        tunable=tunable_train_nodes_columns_disparity,\n",
    "        nodes=train_nodes,\n",
    "        file_name=f\"train_fixed_vs_tunable_{experiment}.png\",\n",
    "        sorted_nodes=sorted_nodes_train,\n",
    "        target=target,\n",
    "        title=\"Comparison between the Train disparity of the baseline, fixed and tunable Lambda (LAST FL ROUND)\",\n",
    "    )\n",
    "    plot_lambda_with_std(\n",
    "        fixed=df_fixed,\n",
    "        tunable=df_tunable,\n",
    "        file_name=f\"train_lambda_{experiment}.png\",\n",
    "        train_nodes_columns_lambda=train_nodes_columns_lambda,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_link = \"/lucacorbucci/FL_fairness/runs/f57qbv4s\"\n",
    "# fixed_link = \"/lucacorbucci/FL_fairness/runs/lvagj1io\"\n",
    "# tunable_link = \"/lucacorbucci/FL_fairness/runs/bthvijqh\"\n",
    "# df_baseline = get_df(baseline_link)\n",
    "# df_fixed = get_df(fixed_link)\n",
    "# df_tunable = get_df(tunable_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 25 Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_plot(\n",
    "    baseline_link=\"/lucacorbucci/FL_PUF_Sweep/runs/vgyi8ygu\",\n",
    "    fixed_link=\"/lucacorbucci/FL_PUF_Sweep/runs/fe9xmlyf\",\n",
    "    tunable_link=\"/lucacorbucci/FL_PUF_Sweep/runs/b0wianme\",\n",
    "    experiment=\"50_50\",\n",
    "    target=0.05,\n",
    "    # df_baseline=df_baseline,\n",
    "    # df_fixed=df_fixed,\n",
    "    # df_tunable=df_tunable,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_and_plot(\n",
    "#     baseline_link=\"/lucacorbucci/FL_PUF_Sweep/runs/61cj7ia9\",\n",
    "#     fixed_link=\"/lucacorbucci/FL_PUF_Sweep/runs/60o6jjbc\",\n",
    "#     tunable_link=None,  # \"/lucacorbucci/FL_PUF_Sweep/runs/b0wianme\",\n",
    "#     experiment=\"50_50\",\n",
    "#     target=0.05,\n",
    "#     # df_baseline=df_baseline,\n",
    "#     # df_fixed=df_fixed,\n",
    "#     # df_tunable=df_tunable,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuovo Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_plot(\n",
    "    baseline_link=\"/lucacorbucci/FL_PUF_Sweep/runs/vo395z17\",\n",
    "    fixed_link=\"/lucacorbucci/FL_PUF_Sweep/runs/wf5o04al\",\n",
    "    tunable_link=\"/lucacorbucci/FL_fairness/runs/j0hv6tlh\",\n",
    "    experiment=\"50_50\",\n",
    "    target=0.1,\n",
    "    # df_baseline=df_baseline,\n",
    "    # df_fixed=df_fixed,\n",
    "    # df_tunable=df_tunable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30% Balanced 70% Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 balanced 50 unblalanced\n",
    "process_and_plot(\n",
    "    baseline_link=\"/lucacorbucci/FL_fairness/runs/f57qbv4s\",\n",
    "    fixed_link=\"/lucacorbucci/FL_fairness/runs/lvagj1io\",\n",
    "    tunable_link=\"/lucacorbucci/FL_fairness/runs/bthvijqh\",\n",
    "    experiment=\"30_70\",\n",
    "    target=0.05,\n",
    "    # df_baseline=df_baseline,\n",
    "    # df_fixed=df_fixed,\n",
    "    # df_tunable=df_tunable,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_link=\"/lucacorbucci/FL_fairness/runs/ng91bamk\"\n",
    "# fixed_link=\"/lucacorbucci/FL_fairness/runs/2jjx2hjh\"\n",
    "# tunable_link=\"/lucacorbucci/FL_fairness/runs/xunyy81c\"\n",
    "# df_baseline = get_df(baseline_link)\n",
    "# df_fixed = get_df(fixed_link)\n",
    "# df_tunable = get_df(tunable_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non IID\n",
    "process_and_plot(\n",
    "    baseline_link=\"/lucacorbucci/FL_fairness/runs/ng91bamk\",\n",
    "    fixed_link=\"/lucacorbucci/FL_fairness/runs/2jjx2hjh\",\n",
    "    tunable_link=\"/lucacorbucci/FL_fairness/runs/xunyy81c\",\n",
    "    experiment=\"non_iid_celeba\",\n",
    "    target=0.1,\n",
    "    # df_baseline=df_baseline,\n",
    "    # df_fixed=df_fixed,\n",
    "    # df_tunable=df_tunable,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50% Balanced 50% Unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 balanced 50 unbalanced\n",
    "process_and_plot(\n",
    "    baseline_link=\"/lucacorbucci/FL_PUF_Sweep/runs/vgyi8ygu\",\n",
    "    fixed_link=\"/lucacorbucci/FL_fairness/runs/ewo63t3s\",\n",
    "    tunable_link=\"/lucacorbucci/FL_fairness/runs/gprysgcv\",\n",
    "    experiment=\"50_50\",\n",
    "    target=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dutch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 balanced 50 unbalanced\n",
    "process_and_plot(\n",
    "    baseline_link=\"/lucacorbucci/FL_PUF_Sweep/runs/vo395z17\",\n",
    "    fixed_link=\"/lucacorbucci/FL_PUF_Sweep/runs/wf5o04al\",\n",
    "    tunable_link=\"/lucacorbucci/FL_PUF_Sweep/runs/tpwzgoep\",\n",
    "    experiment=\"target_01_dutch_distribution_4\",\n",
    "    target=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 50 balanced 50 unbalanced\n",
    "# process_and_plot(\n",
    "#     baseline_link=\"/lucacorbucci/FL_PUF_Sweep/runs/fuwfcfb0\",\n",
    "#     fixed_link=\"/lucacorbucci/FL_PUF_Sweep/runs/y50gn8el\",\n",
    "#     tunable_link=\"/lucacorbucci/FL_PUF_Sweep/runs/4nosq9xd\",\n",
    "#     experiment=\"target_005\",\n",
    "#     target=0.05,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dutch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 50 balanced 50 unbalanced\n",
    "# process_and_plot(\n",
    "#     baseline_link=\"/lucacorbucci/FL_PUF_Sweep/runs/wh898buc\",\n",
    "#     fixed_link=\"/lucacorbucci/FL_PUF_Sweep/runs/ts03o4m8\",\n",
    "#     tunable_link=\"/lucacorbucci/FL_PUF_Sweep/runs/3ppsttus\",\n",
    "#     experiment=\"target_01\",\n",
    "#     target=0.1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_link=\"/lucacorbucci/FL_fairness/runs/ydjnxo29\"\n",
    "# df_baseline = get_df(baseline_link)\n",
    "# baseline_test_nodes_columns_disparity_dataset = remove_nan(test_nodes_columns_disparity_dataset, df_baseline)\n",
    "\n",
    "# nodes_with_disparity = {}\n",
    "# for node_name, column in zip(test_nodes, baseline_test_nodes_columns_disparity_dataset.columns):\n",
    "#     # get the last value of the column\n",
    "#     nodes_with_disparity[node_name] = baseline_test_nodes_columns_disparity_dataset[column].iloc[-1]\n",
    "# # dump nodes_with_disparity.values() in a file\n",
    "# dill.dump(nodes_with_disparity, open(\"nodes_with_disparity_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_link=\"/lucacorbucci/FL_fairness/runs/ydjnxo29\"\n",
    "# df_baseline = get_df(baseline_link)\n",
    "# baseline_test_nodes_columns_disparity_dataset = remove_nan(test_nodes_columns_disparity_dataset, df_baseline)\n",
    "\n",
    "# nodes_with_disparity = {}\n",
    "# for node_name, column in zip(test_nodes, baseline_test_nodes_columns_disparity_dataset.columns):\n",
    "#     # get the last value of the column\n",
    "#     nodes_with_disparity[node_name] = baseline_test_nodes_columns_disparity_dataset[column].iloc[-1]\n",
    "# # dump nodes_with_disparity.values() in a file\n",
    "# dill.dump(nodes_with_disparity, open(\"nodes_with_disparity_train.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable_link = \"/lucacorbucci/FL_fairness/runs/gprysgcv\"\n",
    "# df_tunable = get_df(tunable_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_columns = df_tunable[train_nodes_columns]\n",
    "# disparity_columns_before = df_tunable[train_nodes_columns_disparity_before]\n",
    "# disparity_columns_after = df_tunable[train_nodes_columns_disparity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambdas = {}\n",
    "# for column_name in lambda_columns.columns:\n",
    "#     node_name = str(column_name.split(\" \")[2])\n",
    "#     lambdas[node_name] = list(lambda_columns[column_name].dropna())\n",
    "\n",
    "# disparities_before = {}\n",
    "# for column_name in disparity_columns_before.columns:\n",
    "#     node_name = str(column_name.split(\" \")[2])\n",
    "#     disparities_before[node_name] = list(disparity_columns_before[column_name].dropna())\n",
    "\n",
    "# disparities_after = {}\n",
    "# for column_name in disparity_columns_after.columns:\n",
    "#     node_name = str(column_name.split(\" \")[2])\n",
    "#     disparities_after[node_name] = list(disparity_columns_after[column_name].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the lambdas\n",
    "# figure(figsize=(25, 8), dpi=80)\n",
    "# ax = plt.axes()\n",
    "# ax.yaxis.grid()\n",
    "# for node_name, values in lambdas.items():\n",
    "#     if int(node_name) <= 75:\n",
    "#         plt.plot(values, linewidth=2, label=node_name)\n",
    "# plt.xlabel(\"FL Rounds\")\n",
    "# plt.ylabel(\"Lambda\")\n",
    "# plt.title(f\"Lambda for each unfair node\")\n",
    "# plt.savefig(\"Lambda_for_each_unfair_node.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the lambdas\n",
    "# figure(figsize=(25, 8), dpi=80)\n",
    "# ax = plt.axes()\n",
    "# ax.yaxis.grid()\n",
    "# for node_name, values in lambdas.items():\n",
    "#     if int(node_name) > 75:\n",
    "#         plt.plot(values, linewidth=2, label=node_name)\n",
    "# plt.xlabel(\"FL Rounds\")\n",
    "# plt.ylabel(\"Lambda\")\n",
    "# plt.title(f\"Lambda for each fair node\")\n",
    "# plt.savefig(\"Lambda_for_each_fair_node.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the lambdas\n",
    "# figure(figsize=(25, 8), dpi=80)\n",
    "# ax = plt.axes()\n",
    "# ax.yaxis.grid()\n",
    "# for node_name, values in lambdas.items():\n",
    "#     if int(node_name) <= 75:\n",
    "#         plt.plot(values, linewidth=2, label=node_name)\n",
    "# plt.xlabel(\"FL Rounds\")\n",
    "# plt.ylabel(\"Lambda\")\n",
    "# plt.title(f\"Disparity Before the local epochs for each unfair node\")\n",
    "# plt.savefig(\"disparity_before_for_each_unfair_node.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the lambdas\n",
    "# figure(figsize=(25, 8), dpi=80)\n",
    "# ax = plt.axes()\n",
    "# ax.yaxis.grid()\n",
    "# for node_name, values in lambdas.items():\n",
    "#     if int(node_name) > 75:\n",
    "#         plt.plot(values, linewidth=2, label=node_name)\n",
    "# plt.xlabel(\"FL Rounds\")\n",
    "# plt.ylabel(\"Lambda\")\n",
    "# plt.title(f\"Disparity Before the local epochs for each fair node\")\n",
    "# plt.savefig(\"disparity_before_for_each_fair_node.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the lambdas\n",
    "# figure(figsize=(25, 8), dpi=80)\n",
    "# ax = plt.axes()\n",
    "# ax.yaxis.grid()\n",
    "# for node_name, values in lambdas.items():\n",
    "#     if int(node_name) <= 75:\n",
    "#         plt.plot(values, linewidth=2, label=node_name)\n",
    "# plt.xlabel(\"FL Rounds\")\n",
    "# plt.ylabel(\"Lambda\")\n",
    "# plt.title(f\"Disparity After the local epochs for each unfair node\")\n",
    "# plt.savefig(\"disparity_after_for_each_unfair_node.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the lambdas\n",
    "# figure(figsize=(25, 8), dpi=80)\n",
    "# ax = plt.axes()\n",
    "# ax.yaxis.grid()\n",
    "# for node_name, values in lambdas.items():\n",
    "#     if int(node_name) > 75:\n",
    "#         plt.plot(values, linewidth=2, label=node_name)\n",
    "# plt.xlabel(\"FL Rounds\")\n",
    "# plt.ylabel(\"Lambda\")\n",
    "# plt.title(f\"Disparity After the local epochs for each fair node\")\n",
    "# plt.savefig(\"disparity_after_for_each_fair_node.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
