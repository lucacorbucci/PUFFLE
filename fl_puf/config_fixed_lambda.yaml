program: main.py
method: random
metric:
  name: Custom_metric
  goal: maximize
early_terminate:
  type: hyperband
  min_iter: 3
parameters:
  epochs:
    min: 1
    max: 5
  batch_size:
    min: 128
    max: 512
  lr:
    min: 0.0001
    max: 0.1
  optimizer:
    values: ["adam", "sgd", "adamw"]
  node_shuffle_seed:
    min: 1
    max: 1000000000
  starting_lambda_value:
    min: 0.4
    max: 0.8

command:
  - ${env}
  - poetry 
  - run 
  - python
  - ${program}
  - ${args}
  - --dataset
  - celeba
  - --num_rounds
  - 100
  - --num_client_cpus
  - 1
  - --num_client_gpus
  - 0.2
  - --pool_size
  - 150
  - --sampled_clients 
  - 0.15
  - --sampled_clients_test 
  - 0.20
  - --sampled_clients_validation 
  - 0.34
  - --train_csv
  - unfair_train
  - --debug
  - True 
  - --base_path
  - ../data 
  - --DPL 
  - True 
  - --private 
  - True 
  - --delta 
  - 1e-4 
  - --partition_type 
  - non_iid 
  - --alpha 
  - 5 
  - --seed 
  - 41 
  - --wandb 
  - True 
  - --sweep
  - True
  - --noise_multiplier 
  - 0
  - --clipping
  - 100000.0
  - --training_nodes 
  - 0.47
  - --validation_nodes 
  - 0.20
  - --test_nodes 
  - 0.335
  - --starting_lambda_mode 
  - no_tuning