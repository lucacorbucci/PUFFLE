program: ../../../main.py
method: bayes
metric:
  name: Custom_metric
  goal: maximize
early_terminate:
  type: hyperband
  min_iter: 5
parameters:
  epochs:
    min: 1
    max: 5
  batch_size:
    min: 32
    max: 512
  lr:
    min: 0.0001
    max: 0.1
  optimizer:
    values: ["adam", "sgd"]
  alpha_target_lambda:
    min: 0.1
    max: 4.0
  momentum:
    min: 0.0
    max: 0.9
  node_shuffle_seed:
    min: 1
    max: 1000000000
  weight_decay_lambda:
    min: 0.001
    max: 1.0
  clipping:
    min: 1.0
    max: 20.0

command:
  - ${env}
  - poetry 
  - run 
  - python
  - ${program}
  - ${args}
  - --dataset
  - celeba
  - --num_rounds
  - 39
  - --num_client_cpus
  - 1
  - --num_client_gpus
  - 0.05
  - --pool_size
  - 150
  - --sampled_clients 
  - 0.34
  - --sampled_clients_test 
  - 0.5
  - --sampled_clients_validation 
  - 0.5
  - --train_csv
  - original_merged
  - --debug
  - False
  - --base_path
  - ../../../data 
  - --dataset_path 
  - ../../../data/celeba
  - --private 
  - True 
  - --DPL 
  - True 
  - --seed 
  - 41 
  - --wandb 
  - True 
  - --sweep
  - True
  - --training_nodes 
  - 0.40
  - --validation_nodes 
  - 0.27
  - --test_nodes 
  - 0.335
  - --starting_lambda_mode 
  - disparity
  - --update_lambda 
  - True
  - --target 
  - 0.05
  - --partition_type 
  - representative 
  - --group_to_reduce 
  - 1 
  - 1 
  - --group_to_increment 
  - 0 
  - 1 
  - --number_of_samples_per_node 
  - 1350
  - --ratio_unfair_nodes 
  - 0.5 
  - --ratio_unfairness 
  - 0.9 
  - 0.9
  - --epsilon_lambda 
  - 0.5
  - --epsilon
  - 7
  - --epsilon_statistics
  - 0.5
  - --one_group_nodes
  - True