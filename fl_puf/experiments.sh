# Baseline without Privacy and without DPL
poetry run python main.py --batch_size=183 --epochs=3 --lr=0.0642664139412466 --node_shuffle_seed=109460240 --optimizer=sgd --dataset celeba --num_rounds 50 --num_client_cpus 1 --num_client_gpus 0.2 --pool_size 150 --sampled_clients 0.10 --sampled_clients_test 1.0 --train_csv unfair_train --debug True --base_path ../data --private True --delta 0.0001 --partition_type non_iid --alpha 5 --seed 41 --wandb True --noise_multiplier 0 --clipping 100000 --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode disparity 

# DPL 
Command
poetry run python main.py --alpha_target_lambda=1.3845378853715764 --batch_size=130 --epochs=3 --lr=0.07883081879897207 --momentum=0.3242433233377496 --node_shuffle_seed=838706845 --optimizer=adam --weight_decay_lambda=0.717999227519428 --dataset celeba --num_rounds 50 --num_client_cpus 1 --num_client_gpus 0.2 --pool_size 150 --sampled_clients 0.10 --sampled_clients_test 1.0 --train_csv unfair_train --debug True --base_path ../data --DPL True --private True --delta 0.0001 --partition_type non_iid --alpha 5 --seed 41 --wandb True --target 0.1 --sweep True --noise_multiplier 0 --clipping 100000 --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode disparity


# Fixed Lambda
poetry run python main.py --batch_size=177 --epochs=5 --lr=0.08886715247049416 --node_shuffle_seed=264337429 --optimizer=sgd --starting_lambda_value=0.6589416003838935 --dataset celeba --num_rounds 100 --num_client_cpus 1 --num_client_gpus 0.2 --pool_size 150 --sampled_clients 0.10 --sampled_clients_test 1.0 --train_csv unfair_train --debug True --base_path ../data --DPL True --private True --delta 0.0001 --partition_type non_iid --alpha 5 --seed 41 --wandb True --sweep True --noise_multiplier 0 --clipping 100000 --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode no_tuning