{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from DPL.RegularizationLoss import RegularizationLoss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from fl_puf.Utils.tabular_data_loader import get_tabular_numpy_dataset\n",
    "import os\n",
    "from DPL.Datasets.dutch import TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_violation_with_argmax(\n",
    "        predictions_argmax: torch.tensor,\n",
    "        sensitive_attribute_list: torch.tensor,\n",
    "        current_target: int,\n",
    "        current_sensitive_feature: int,\n",
    "    ):\n",
    "        counter_sensitive_features = Counter(\n",
    "            [item.item() for item in sensitive_attribute_list]\n",
    "        )\n",
    "\n",
    "        opposite_sensitive_feature = 0 if current_sensitive_feature == 1 else 1\n",
    "\n",
    "        Z_eq_z_argmax = counter_sensitive_features[current_sensitive_feature]\n",
    "        Z_not_eq_z_argmax = counter_sensitive_features[opposite_sensitive_feature]\n",
    "        Y_eq_k_and_Z_eq_z_argmax = 0\n",
    "        Y_eq_k_and_Z_not_eq_z_argmax = 0\n",
    "\n",
    "        for prediction, sensitive_feature in zip(\n",
    "            predictions_argmax, sensitive_attribute_list\n",
    "        ):\n",
    "            if (\n",
    "                prediction == current_target\n",
    "                and sensitive_feature == current_sensitive_feature\n",
    "            ):\n",
    "                Y_eq_k_and_Z_eq_z_argmax += 1\n",
    "            elif (\n",
    "                prediction == current_target\n",
    "                and sensitive_feature == opposite_sensitive_feature\n",
    "            ):\n",
    "                Y_eq_k_and_Z_not_eq_z_argmax += 1\n",
    "\n",
    "        if Z_eq_z_argmax == 0 and Z_not_eq_z_argmax != 0:\n",
    "            return np.abs(Y_eq_k_and_Z_not_eq_z_argmax / Z_not_eq_z_argmax).item()\n",
    "        elif Z_eq_z_argmax != 0 and Z_not_eq_z_argmax == 0:\n",
    "            return np.abs(Y_eq_k_and_Z_eq_z_argmax / Z_eq_z_argmax).item()\n",
    "        else:\n",
    "            # print(\n",
    "            #     f\"Y_eq_k_and_Z_eq_z_argmax: {Y_eq_k_and_Z_eq_z_argmax}, Z_eq_z_argmax: {Z_eq_z_argmax}, Y_eq_k_and_Z_not_eq_z_argmax: {Y_eq_k_and_Z_not_eq_z_argmax}, Z_not_eq_z_argmax: {Z_not_eq_z_argmax}\"\n",
    "            # )\n",
    "            # print(f\"{Y_eq_k_and_Z_eq_z_argmax} / {Z_eq_z_argmax} - {Y_eq_k_and_Z_not_eq_z_argmax} / {Z_not_eq_z_argmax} = {np.abs(Y_eq_k_and_Z_eq_z_argmax / Z_eq_z_argmax - Y_eq_k_and_Z_not_eq_z_argmax / Z_not_eq_z_argmax).item()}\")\n",
    "            return  Y_eq_k_and_Z_eq_z_argmax / Z_eq_z_argmax - Y_eq_k_and_Z_not_eq_z_argmax / Z_not_eq_z_argmax\n",
    "            \n",
    "# plot the bar plot of the disparities\n",
    "def plot_bar_plot(title: str, disparities: list, nodes: list):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.bar(range(len(disparities)), disparities)\n",
    "    plt.xticks(range(len(nodes)), nodes)\n",
    "    plt.title(title)\n",
    "    # add a vertical line on xtick=75 \n",
    "    plt.axvline(x=75, color='r', linestyle='--')\n",
    "    plt.xticks(rotation=90)\n",
    "    # plt.show()\n",
    "    # font size x axis \n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    plt.savefig(f'./{title}.png')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def compute_disparities(nodes):\n",
    "        \n",
    "    disparities = []\n",
    "    for node in nodes:\n",
    "        dataset = torch.load(f'./data/Tabular/adult/federated/{node}/train.pt')\n",
    "        max_disparity = np.max(\n",
    "                [\n",
    "                    RegularizationLoss().compute_violation_with_argmax(\n",
    "                        predictions_argmax=dataset.targets,\n",
    "                        sensitive_attribute_list=dataset.sensitive_features,\n",
    "                        current_target=target,\n",
    "                        current_sensitive_feature=sv,\n",
    "                    )\n",
    "                    for target in range(0, 1)\n",
    "                    for sv in range(0, 1)\n",
    "                ]\n",
    "            )\n",
    "        disparities.append(max_disparity)\n",
    "    return disparities\n",
    "\n",
    "def compute_disparities_per_group(nodes, target, sv, client_data):\n",
    "    disparities = []\n",
    "    for client in client_data:\n",
    "        \n",
    "        sensive_features = list(client[\"z\"])\n",
    "        predictions_argmax = list(client[\"y\"])\n",
    "        disparity = compute_violation_with_argmax(\n",
    "                            predictions_argmax=predictions_argmax,\n",
    "                            sensitive_attribute_list=sensive_features,\n",
    "                            current_target=target,\n",
    "                            current_sensitive_feature=sv,\n",
    "                        )\n",
    "\n",
    "        disparities.append(disparity)\n",
    "    return disparities\n",
    "\n",
    "def find_disparity(groups_balance_factor, priv_balance_factor, folder_name=None, do_iid_split=False):\n",
    "    fed_dir, client_data = prepare_tabular_data(\n",
    "        dataset_path=\"./data/Tabular/adult/\",\n",
    "        dataset_name=\"adult\",\n",
    "        groups_balance_factor=groups_balance_factor,\n",
    "        priv_balance_factor=priv_balance_factor,\n",
    "        do_iid_split=do_iid_split,\n",
    "    )\n",
    "    clients_disparity, clients_name = get_disparity(client_data)\n",
    "    # plot the disparity for each client\n",
    "    print(\n",
    "        f\"Current setup groups_balance_factor={groups_balance_factor}, priv_balance_factor={priv_balance_factor}\"\n",
    "    )\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.bar(clients_name, clients_disparity)\n",
    "    plt.xlabel(\"Client\")\n",
    "    plt.ylabel(\"Disparity\")\n",
    "    plt.title(\"Disparity per client\")\n",
    "    plt.tight_layout()\n",
    "    if folder_name:\n",
    "        plt.savefig(\n",
    "            f\"./experiments/{folder_name}/disparity_nodes_{groups_balance_factor}_{priv_balance_factor}.png\"\n",
    "        )\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    client_dataset_size = []\n",
    "\n",
    "    for client in client_data:\n",
    "        client_dataset_size.append(len(client[\"x\"]))\n",
    "    plot_amount_of_data(client_dataset_size)\n",
    "    return client_data\n",
    "\n",
    "def get_disparity(client_data):\n",
    "    clients_disparity = []\n",
    "    client_names = []\n",
    "    for client_name, client in enumerate(client_data):\n",
    "        max_disparity = 0\n",
    "        y = list(client[\"y\"])\n",
    "        z = list(client[\"z\"])\n",
    "        for target in [0, 1]:\n",
    "            for sensitive_value in [0, 1]:\n",
    "                max_disparity = max(\n",
    "                    RegularizationLoss().compute_violation_with_argmax(\n",
    "                        predictions_argmax=y,\n",
    "                        sensitive_attribute_list=z,\n",
    "                        current_target=target,\n",
    "                        current_sensitive_feature=sensitive_value,\n",
    "                    ),\n",
    "                    max_disparity,\n",
    "                )\n",
    "        clients_disparity.append(max_disparity)\n",
    "        client_names.append(client_name)\n",
    "    return clients_disparity, client_names\n",
    "\n",
    "def plot_amount_of_data(client_dataset_size):\n",
    "    # plot a barplot of the amount of sensitive feature 0 per client and the amount \n",
    "    # of sensitive feature 1 per client in the same plot\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    plt.bar(range(len(client_dataset_size)), client_dataset_size)\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Clients\")\n",
    "    plt.ylabel(\"Amount of samples\")\n",
    "    plt.title(\"Amount of samples per client\")\n",
    "    # font size 20\n",
    "    plt.rcParams.update({'font.size': 20}) \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tabular_data(\n",
    "    dataset_path: str,\n",
    "    dataset_name: str,\n",
    "    groups_balance_factor: float,\n",
    "    priv_balance_factor: float,\n",
    "    do_iid_split: bool = False,\n",
    "):\n",
    "    client_data, N_is, props_positive = get_tabular_data(\n",
    "        num_clients=150,\n",
    "        do_iid_split=do_iid_split,\n",
    "        groups_balance_factor=groups_balance_factor,  # fraction of privileged clients ->\n",
    "        priv_balance_factor=priv_balance_factor,  # fraction of priv samples the privileged clients should have\n",
    "        dataset_name=\"adult\",\n",
    "        num_sensitive_features=1,\n",
    "        dataset_path=dataset_path,\n",
    "    )\n",
    "    # remove the old files in the data folder\n",
    "    os.system(f\"rm -rf {dataset_path}/federated/*\")\n",
    "    for client_name, client in enumerate(client_data):\n",
    "        # Append 1 to each samples\n",
    "\n",
    "        custom_dataset = TabularDataset(\n",
    "            x=np.hstack((client[\"x\"], np.ones((client[\"x\"].shape[0], 1)))).astype(\n",
    "                np.float32\n",
    "            ),\n",
    "            z=client[\"z\"].astype(np.float32),\n",
    "            y=client[\"y\"].astype(np.float32),\n",
    "        )\n",
    "        # Create the folder for the user client_name\n",
    "        os.system(f\"mkdir {dataset_path}/federated/{client_name}\")\n",
    "        # store the dataset in the client folder with the name \"train.pt\"\n",
    "        torch.save(\n",
    "            custom_dataset,\n",
    "            f\"{dataset_path}/federated/{client_name}/train.pt\",\n",
    "        )\n",
    "    fed_dir = f\"{dataset_path}/federated\"\n",
    "    return fed_dir, client_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_tabular_data(\n",
    "    num_clients: int,\n",
    "    do_iid_split: bool,\n",
    "    groups_balance_factor: float,\n",
    "    priv_balance_factor: float,\n",
    "    dataset_name: str,\n",
    "    num_sensitive_features: int,\n",
    "    dataset_path=None,\n",
    "):\n",
    "    X, z, y = get_tabular_numpy_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        num_sensitive_features=num_sensitive_features,\n",
    "        dataset_path=dataset_path,\n",
    "    )\n",
    "    print(f\"Data shapes: x={X.shape}, y={y.shape}, z={z.shape}\")\n",
    "    # Prepare training data held by each client\n",
    "    client_data, N_is, props_positive = generate_clients_biased_data_mod(\n",
    "        x=X,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        M=num_clients,\n",
    "        do_iid_split=do_iid_split,\n",
    "        clients_balance_factor=groups_balance_factor,\n",
    "        priv_balance_factor=priv_balance_factor,\n",
    "    )\n",
    "    return client_data, N_is, props_positive\n",
    "\n",
    "def compute_violation_with_argmax(\n",
    "        predictions_argmax: torch.tensor,\n",
    "        sensitive_attribute_list: torch.tensor,\n",
    "        current_target: int,\n",
    "        current_sensitive_feature: int,\n",
    "    ):\n",
    "        counter_sensitive_features = Counter(\n",
    "            [item.item() for item in sensitive_attribute_list]\n",
    "        )\n",
    "\n",
    "        opposite_sensitive_feature = 0 if current_sensitive_feature == 1 else 1\n",
    "\n",
    "        Z_eq_z_argmax = counter_sensitive_features[current_sensitive_feature]\n",
    "        Z_not_eq_z_argmax = counter_sensitive_features[opposite_sensitive_feature]\n",
    "        Y_eq_k_and_Z_eq_z_argmax = 0\n",
    "        Y_eq_k_and_Z_not_eq_z_argmax = 0\n",
    "\n",
    "        for prediction, sensitive_feature in zip(\n",
    "            predictions_argmax, sensitive_attribute_list\n",
    "        ):\n",
    "            if (\n",
    "                prediction == current_target\n",
    "                and sensitive_feature == current_sensitive_feature\n",
    "            ):\n",
    "                Y_eq_k_and_Z_eq_z_argmax += 1\n",
    "            elif (\n",
    "                prediction == current_target\n",
    "                and sensitive_feature == opposite_sensitive_feature\n",
    "            ):\n",
    "                Y_eq_k_and_Z_not_eq_z_argmax += 1\n",
    "\n",
    "        if Z_eq_z_argmax == 0 and Z_not_eq_z_argmax != 0:\n",
    "            return np.abs(Y_eq_k_and_Z_not_eq_z_argmax / Z_not_eq_z_argmax).item()\n",
    "        elif Z_eq_z_argmax != 0 and Z_not_eq_z_argmax == 0:\n",
    "            return np.abs(Y_eq_k_and_Z_eq_z_argmax / Z_eq_z_argmax).item()\n",
    "        else:\n",
    "            print(\n",
    "                f\"Y_eq_k_and_Z_eq_z_argmax: {Y_eq_k_and_Z_eq_z_argmax}, Z_eq_z_argmax: {Z_eq_z_argmax}, Y_eq_k_and_Z_not_eq_z_argmax: {Y_eq_k_and_Z_not_eq_z_argmax}, Z_not_eq_z_argmax: {Z_not_eq_z_argmax}\"\n",
    "            )\n",
    "            print(f\"{Y_eq_k_and_Z_eq_z_argmax} / {Z_eq_z_argmax} - {Y_eq_k_and_Z_not_eq_z_argmax} / {Z_not_eq_z_argmax} = {np.abs(Y_eq_k_and_Z_eq_z_argmax / Z_eq_z_argmax - Y_eq_k_and_Z_not_eq_z_argmax / Z_not_eq_z_argmax).item()}\")\n",
    "            return  Y_eq_k_and_Z_eq_z_argmax / Z_eq_z_argmax - Y_eq_k_and_Z_not_eq_z_argmax / Z_not_eq_z_argmax\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG \n",
    "def compute_disparities_debug(nodes):\n",
    "    disparities = []\n",
    "    samples = []\n",
    "    for node in nodes:\n",
    "        samples.append(len(node))\n",
    "        max_disparity = [\n",
    "                    RegularizationLoss().compute_violation_with_argmax(\n",
    "                        predictions_argmax=[sample['y'] for sample in node],\n",
    "                        sensitive_attribute_list=[sample['z'] for sample in node],\n",
    "                        current_target=target,\n",
    "                        current_sensitive_feature=sv,\n",
    "                    )\n",
    "                    for target in [0, 1]\n",
    "                    for sv in [0, 1]\n",
    "                ]\n",
    "        disparities.append(np.max(max_disparity))\n",
    "    print(f\"Mean of disparity {np.mean(disparities)} - std {np.std(disparities)} - Mean number o samples {np.mean(samples)}\")\n",
    "    return disparities\n",
    "\n",
    "\n",
    "def compute_disparities_per_group_debug(nodes, target, sv, client_data):\n",
    "    disparities = []\n",
    "    for client in client_data:\n",
    "        \n",
    "        sensive_features = [sample[\"z\"] for sample in client]\n",
    "        predictions_argmax = [sample[\"y\"] for sample in client]\n",
    "        disparity = compute_violation_with_argmax(\n",
    "                            predictions_argmax=predictions_argmax,\n",
    "                            sensitive_attribute_list=sensive_features,\n",
    "                            current_target=target,\n",
    "                            current_sensitive_feature=sv,\n",
    "                        )\n",
    "\n",
    "        disparities.append(disparity)\n",
    "    return disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egalitarian point of view: equal representation of each subgroup 100 samples->25 per group\n",
    "# representative diversity: each node has the same ratio that we are observing in the dataset \n",
    "\n",
    "# representative diversity + unfair nodes that exagerates the disparity\n",
    "# (ratio_fair_nodes, ratio_unfair_nodes, number_of_samples_per_node)\n",
    "\n",
    "# one more parameter: how many fair ndoes want to have more 0 or more 1s. -> expeirments with nodes that go in one direction and nodes\n",
    "# that go to other direction\n",
    "# another parameter -> egalitarian vs representative diversity\n",
    "\n",
    "# Othre idea: solve the problem of opposite directons with an alternative to DPL.\n",
    "\n",
    "def egalitarian_approach(X, y, z, num_nodes, number_of_samples_per_node=None):\n",
    "    \"\"\"\n",
    "    With this approach we want to distribute the data among the nodes in an egalitarian way.\n",
    "    This means that each node has the same amount of data and the same ratio of each group\n",
    "\n",
    "    params:\n",
    "    X: numpy array of shape (N, D) where N is the number of samples and D is the number of features\n",
    "    y: numpy array of shape (N, ) where N is the number of samples. Here we have the samples labels\n",
    "    z: numpy array of shape (N, ) where N is the number of samples. Here we have the samples sensitive features\n",
    "    num_nodes: number of nodes to generate\n",
    "    number_of_samples_per_node: number of samples that we want in each node. Can be None, in this case we just use \n",
    "        len(y)//num_nodes\n",
    "    \"\"\"\n",
    "    combinations = [(target, sensitive_value) for target, sensitive_value in zip(y, z)]\n",
    "    possible_combinations = set(combinations)\n",
    "    data = {}\n",
    "    for combination, x_, y_, z_ in zip(combinations, X, y, z):\n",
    "        if combination not in data:\n",
    "            data[combination] = []\n",
    "        data[combination].append({\"x\": x_, \"y\": y_, \"z\": z_})\n",
    "\n",
    "    samples_from_each_group = min(list(Counter(combinations).values())) // num_nodes\n",
    "\n",
    "    if number_of_samples_per_node:\n",
    "        assert samples_from_each_group*len(possible_combinations) >= number_of_samples_per_node, \"Too many samples per node, choose a different number of samples per node\"\n",
    "        if samples_from_each_group*len(possible_combinations) >= number_of_samples_per_node:\n",
    "            to_be_removed = (samples_from_each_group*len(possible_combinations) - number_of_samples_per_node)//len(possible_combinations)\n",
    "            samples_from_each_group -= to_be_removed\n",
    "            \n",
    "\n",
    "    # create the nodes\n",
    "    nodes = []\n",
    "    for i in range(num_nodes):\n",
    "        nodes.append([])\n",
    "        # fill the nodes\n",
    "        for combination in data:\n",
    "            nodes[i].extend(data[combination][:samples_from_each_group])\n",
    "            data[combination] = data[combination][samples_from_each_group:]\n",
    "\n",
    "    \n",
    "    return nodes, data\n",
    "\n",
    "def create_unfair_nodes(fair_nodes: list, nodes_to_unfair: list, remaining_data: dict, group_to_reduce: tuple, group_to_increment:tuple, ratio_unfairness: tuple):\n",
    "    \"\"\"\n",
    "    This function creates the unfair nodes. It takes the nodes that we want to be unfair and the remaining data\n",
    "    and it returns the unfair nodes created by reducing the group_to_reduce and incrementing the group_to_increment\n",
    "    based on the ratio_unfairness\n",
    "\n",
    "    params:\n",
    "    nodes_to_unfair: list of nodes that we want to make unfair\n",
    "    remaining_data: dictionary with the remaining data that we will use to replace the \n",
    "        samples that we remove from the nodes_to_unfair\n",
    "    group_to_reduce: the group that we want to be unfair. For instance, in the case of binary target and binary sensitive value\n",
    "        we could have (0,0), (0,1), (1,0) or (1,1)\n",
    "    group_to_increment: the group that we want to increment. For instance, in the case of binary target and binary sensitive value\n",
    "        we could have (0,0), (0,1), (1,0) or (1,1)\n",
    "    ratio_unfairness: tuple (min, max) where min is the minimum ratio of samples that we want to remove from the group_to_reduce\n",
    "    \"\"\"\n",
    "    #assert remaining_data[group_to_reduce] != [], \"Choose a different group to be unfair\"\n",
    "    # remove the samples from the group that we want to be unfair\n",
    "    unfair_nodes = []\n",
    "    number_of_samples_to_add = []\n",
    "    removed_samples = []\n",
    "\n",
    "    for node in nodes_to_unfair:\n",
    "        node_data = []\n",
    "        count_sensitive_group_samples = 0\n",
    "        # We count how many sample each node has from the group that we want to reduce\n",
    "        for sample in node:\n",
    "            if (sample['y'], sample[\"z\"]) == group_to_reduce:\n",
    "                count_sensitive_group_samples += 1\n",
    "\n",
    "        # We compute the number of samples that we want to remove from the group_to_reduce\n",
    "        # based on the ratio_unfairness\n",
    "        current_ratio = np.random.uniform(ratio_unfairness[0], ratio_unfairness[1])\n",
    "        samples_to_be_removed = int(count_sensitive_group_samples*current_ratio)\n",
    "        number_of_samples_to_add.append(samples_to_be_removed)\n",
    "\n",
    "        for sample in node:\n",
    "            # Now we remove the samples from the group_to_reduce \n",
    "            # and we store them in removed_samples\n",
    "            if (sample['y'], sample[\"z\"]) == group_to_reduce and samples_to_be_removed > 0:\n",
    "                samples_to_be_removed -= 1\n",
    "                removed_samples.append(sample)\n",
    "            else:\n",
    "                node_data.append(sample)\n",
    "        unfair_nodes.append(node_data)\n",
    "\n",
    "    # Now we have to distribute the removed samples among the fair nodes\n",
    "    max_samples_to_add = len(removed_samples)//len(fair_nodes)\n",
    "    for node in fair_nodes:\n",
    "        node.extend(removed_samples[:max_samples_to_add])\n",
    "        removed_samples = removed_samples[max_samples_to_add:]\n",
    "    \n",
    "    \n",
    "    if group_to_increment:\n",
    "        # Now we have to remove the samples from the group_to_increment\n",
    "        # from the fair_nodes based on the number_of_samples_to_add\n",
    "        for node in fair_nodes:\n",
    "            samples_to_remove = sum(number_of_samples_to_add) // len(fair_nodes)\n",
    "            for index, sample in enumerate(node):\n",
    "                if (sample['y'], sample[\"z\"]) == group_to_increment and samples_to_remove > 0:\n",
    "                    if (sample['y'], sample[\"z\"]) not in remaining_data:\n",
    "                        remaining_data[group_to_increment] = []\n",
    "                    remaining_data[group_to_increment].append(sample)\n",
    "                    samples_to_remove -= 1\n",
    "                    node.pop(index)\n",
    "            if sum(number_of_samples_to_add) > 0:\n",
    "                assert samples_to_remove == 0, \"Not enough samples to remove\"\n",
    "        assert sum(number_of_samples_to_add) <= len(remaining_data[group_to_increment]), \"Too many samples to add\"\n",
    "        # now we have to add the same amount of data taken from group_to_unfair\n",
    "        for node, samples_to_add in zip(unfair_nodes, number_of_samples_to_add):\n",
    "            node.extend(remaining_data[group_to_increment][:samples_to_add])\n",
    "            remaining_data[group_to_increment] = remaining_data[group_to_increment][samples_to_add:]\n",
    "\n",
    "    return fair_nodes, unfair_nodes\n",
    "\n",
    "def representative_diversity_approach(X, y, z, num_nodes, number_of_samples_per_node):\n",
    "    \"\"\"\n",
    "    With this approach we want to distribute the data among the nodes in a representative diversity way.\n",
    "    This means that each node has the same ratio of each group that we are observing in the dataset\n",
    "\n",
    "    params:\n",
    "    X: numpy array of shape (N, D) where N is the number of samples and D is the number of features\n",
    "    y: numpy array of shape (N, ) where N is the number of samples. Here we have the samples labels\n",
    "    z: numpy array of shape (N, ) where N is the number of samples. Here we have the samples sensitive features\n",
    "    num_nodes: number of nodes to generate\n",
    "    number_of_samples_per_node: number of samples that we want in each node. Can be None, in this case we just use\n",
    "        len(y)//num_nodes\n",
    "    \"\"\"\n",
    "    samples_per_node = number_of_samples_per_node if number_of_samples_per_node else len(y)//num_nodes\n",
    "    # create the nodes sampling from the dataset wihout replacement\n",
    "    dataset = [{\"x\": x_, \"y\": y_, \"z\": z_} for x_, y_, z_ in zip(X, y, z)]\n",
    "    # shuffle the dataset\n",
    "    np.random.shuffle(dataset)\n",
    "\n",
    "    # Distribute the data among the nodes with a random sample from the dataset \n",
    "    # considering the number of samples per node\n",
    "    nodes = []\n",
    "    for i in range(num_nodes):\n",
    "        nodes.append([])\n",
    "        nodes[i].extend(dataset[:samples_per_node])\n",
    "        dataset = dataset[samples_per_node:]\n",
    "    \n",
    "    remaining_data = {}\n",
    "    \n",
    "    if number_of_samples_per_node:\n",
    "        # Create the dictionary with the remaining data\n",
    "        for sample in dataset:\n",
    "            if (sample['y'], sample['z']) not in remaining_data:\n",
    "                remaining_data[(sample['y'], sample['z'])] = []\n",
    "            remaining_data[(sample['y'], sample['z'])].append(sample)\n",
    "\n",
    "    return nodes, remaining_data\n",
    "\n",
    "def generate_clients_biased_data_mod(\n",
    "    x, \n",
    "    y, \n",
    "    z, \n",
    "    approach:str, \n",
    "    num_nodes:int, \n",
    "    ratio_unfair_nodes:float, \n",
    "    opposite_direction:bool, \n",
    "    ratio_unfairness:tuple, \n",
    "    group_to_reduce:tuple=None, \n",
    "    group_to_increment:tuple=None, \n",
    "    number_of_samples_per_node:int=None, \n",
    "    opposite_group_to_reduce:tuple=None,\n",
    "    opposite_group_to_increment:tuple=None,\n",
    "    opposite_ratio_unfairness:tuple=None,):\n",
    "    \"\"\"\n",
    "    This function generates the data for the clients.\n",
    "\n",
    "    params:\n",
    "    X: numpy array of shape (N, D) where N is the number of samples and D is the number of features\n",
    "    y: numpy array of shape (N, ) where N is the number of samples. Here we have the samples labels\n",
    "    z: numpy array of shape (N, ) where N is the number of samples. Here we have the samples sensitive features\n",
    "    num_nodes: number of nodes to generate\n",
    "    approach: type of approach we want to use to distribute the data among the fair clients. This can be egalitarian or representative\n",
    "    ratio_unfair_nodes: the fraction of unfair clients we want to have in the experiment \n",
    "    opposite_direction: true if we want to allow different nodes to have different majoritiarian classes. For instance, \n",
    "        we could have some nodes with a max disparity that depends on the majority class being 0 and other nodes with a max disparity\n",
    "        that depends on the majority class being 1.\n",
    "    group_to_reduce: the group that we want to be unfair. For instance, in the case of binary target and binary sensitive value \n",
    "        we could have (0,0), (0,1), (1,0) or (1,1)\n",
    "    ratio_unfairness: tuple (min, max) where min is the minimum ratio of samples that we want to remove from the group_to_reduce\n",
    "        and max is the maximum ratio of samples that we want to remove from the group_to_reduce\n",
    "    \"\"\"\n",
    "\n",
    "    # check if the number of samples that we want in each node is\n",
    "    # greater than the number of samples we have in the dataset\n",
    "    if number_of_samples_per_node:\n",
    "        assert number_of_samples_per_node < len(y)//num_nodes, \"Too many samples per node\"\n",
    "    # check if the ratio_fair_nodes is between 0 and 1\n",
    "    assert ratio_unfair_nodes <= 1, \"ratio_unfair_nodes must be less or equal than 1\"\n",
    "    assert ratio_unfair_nodes >= 0, \"ratio_unfair_nodes must be greater or equal than 0\"\n",
    "    assert group_to_reduce != None, \"group_to_reduce must be specified\"\n",
    "    # assert group_to_increment != None, \"group_to_increment must be specified\"\n",
    "    # check if the approach type is egalitarian or representative\n",
    "    assert approach in [\"egalitarian\", \"representative\"], \"Approach must be egalitarian or representative\"\n",
    "\n",
    "    number_unfair_nodes = int(num_nodes*ratio_unfair_nodes)\n",
    "    number_fair_nodes = num_nodes - number_unfair_nodes\n",
    "    if approach == \"egalitarian\":\n",
    "        # first split the data among the nodes in an egalitarian way\n",
    "        # each node has the same amount of data and the same ratio of each group\n",
    "        nodes, remaining_data = egalitarian_approach(X, y, z, num_nodes, number_of_samples_per_node)\n",
    "    else:\n",
    "        nodes, remaining_data = representative_diversity_approach(X, y, z, num_nodes, number_of_samples_per_node)\n",
    "\n",
    "    if opposite_direction:\n",
    "        assert opposite_group_to_reduce != None, \"opposite_group_to_reduce must be specified\"\n",
    "        assert opposite_group_to_increment != None, \"opposite_group_to_increment must be specified\"\n",
    "        group_size = number_unfair_nodes//2\n",
    "        unfair_nodes_direction_1 = create_unfair_nodes(nodes_to_unfair=nodes[number_unfair_nodes:number_unfair_nodes+group_size], remaining_data=remaining_data, group_to_reduce=group_to_reduce, group_to_increment=group_to_increment, ratio_unfairness=ratio_unfairness)\n",
    "        unfair_nodes_direction_2 = create_unfair_nodes(nodes_to_unfair=nodes[number_unfair_nodes+group_size:], remaining_data=remaining_data, group_to_reduce=opposite_group_to_reduce, group_to_increment=opposite_group_to_increment, ratio_unfairness=opposite_ratio_unfairness)\n",
    "        return nodes[0:number_unfair_nodes] + unfair_nodes_direction_1 + unfair_nodes_direction_2\n",
    "    else:\n",
    "        fair_nodes, unfair_nodes = create_unfair_nodes(fair_nodes=nodes[0:number_fair_nodes], nodes_to_unfair=nodes[number_fair_nodes:], remaining_data=remaining_data, group_to_reduce=group_to_reduce, group_to_increment=group_to_increment, ratio_unfairness=ratio_unfairness)\n",
    "        return fair_nodes + unfair_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"./data/Tabular/kdd/KDD-census-income.data\")\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def load_kdd(dataset_path):\n",
    "#     kdd_columns = [\n",
    "#         \"age\",\n",
    "#         \"class of worker\",\n",
    "#         \"industry code\",\n",
    "#         \"occupation code\",\n",
    "#         \"adjusted gross income\",\n",
    "#         \"education\",\n",
    "#         \"wage per hour\",\n",
    "#         \"enrolled in edu inst last wk\",\n",
    "#         \"marital status\",\n",
    "#         \"major industry code\",\n",
    "#         \"major occupation code\",\n",
    "#         \"mace\",\n",
    "#         \"hispanic Origin\",\n",
    "#         \"sex\",\n",
    "#         \"member of a labor union\",\n",
    "#         \"reason for unemployment\",\n",
    "#         \"full or part time employment stat\",\n",
    "#         \"capital gains\",\n",
    "#         \"capital losses\",\n",
    "#         \"divdends from stocks\",\n",
    "#         \"federal income tax liability\",\n",
    "#         \"tax filer status\",\n",
    "#         \"region of previous residence\",\n",
    "#         \"state of previous residence\",\n",
    "#         \"detailed household and family stat\",\n",
    "#         \"detailed household summary in household\",\n",
    "#         \"instance weight\",\n",
    "#         \"migration code-change in msa\",\n",
    "#         \"migration code-change in reg\",\n",
    "#         \"migration code-move within reg\",\n",
    "#         \"live in this house 1 year ago\",\n",
    "#         \"migration prev res in sunbelt\",\n",
    "#         \"num persons worked for employer\",\n",
    "#         \"family members under 18\",\n",
    "#         \"total person earnings\",\n",
    "#         \"country of birth father\",\n",
    "#         \"country of birth mother\",\n",
    "#         \"country of birth self\",\n",
    "#         \"citizenship\",\n",
    "#         \"total person income\",\n",
    "#         \"own business or self employed\",\n",
    "#         \"taxable income amount\",\n",
    "#         \"fill inc questionnaire for veteran's admin\",\n",
    "#         \"veterans benefits\",\n",
    "#         \"weeks worked in year\",\n",
    "#     ]\n",
    "\n",
    "#     feature_columns_kdd = [\n",
    "#         \"age\",\n",
    "#         \"class of worker\",\n",
    "#         \"industry code\",\n",
    "#         \"occupation code\",\n",
    "#         \"adjusted gross income\",\n",
    "#         \"education\",\n",
    "#         \"wage per hour\",\n",
    "#         \"enrolled in edu inst last wk\",\n",
    "#         \"marital status\",\n",
    "#         \"major industry code\",\n",
    "#         \"major occupation code\",\n",
    "#         \"mace\",\n",
    "#         \"hispanic Origin\",\n",
    "#         \"member of a labor union\",\n",
    "#         \"reason for unemployment\",\n",
    "#         \"full or part time employment stat\",\n",
    "#         \"capital gains\",\n",
    "#         \"capital losses\",\n",
    "#         \"divdends from stocks\",\n",
    "#         \"federal income tax liability\",\n",
    "#         \"tax filer status\",\n",
    "#         \"region of previous residence\",\n",
    "#         \"state of previous residence\",\n",
    "#         \"detailed household and family stat\",\n",
    "#         \"detailed household summary in household\",\n",
    "#         \"instance weight\",\n",
    "#         \"migration code-change in msa\",\n",
    "#         \"migration code-change in reg\",\n",
    "#         \"migration code-move within reg\",\n",
    "#         \"live in this house 1 year ago\",\n",
    "#         \"migration prev res in sunbelt\",\n",
    "#         \"num persons worked for employer\",\n",
    "#         \"family members under 18\",\n",
    "#         \"total person earnings\",\n",
    "#         \"country of birth father\",\n",
    "#         \"country of birth mother\",\n",
    "#         \"country of birth self\",\n",
    "#         \"citizenship\",\n",
    "#         \"own business or self employed\",\n",
    "#         \"fill inc questionnaire for veteran's admin\",\n",
    "#         \"veterans benefits\",\n",
    "#         \"weeks worked in year\",\n",
    "#     ]\n",
    "\n",
    "#     kdd_census_df = pd.read_csv(\n",
    "#         dataset_path + \"KDD-census-income.data\",\n",
    "#         names=kdd_columns,\n",
    "#     )\n",
    "\n",
    "#     kdd_census_df = kdd_census_df[kdd_census_df[\"sex\"] != \" Not in universe\"]\n",
    "#     kdd_census_df[\"sex_binary\"] = np.where(kdd_census_df[\"sex\"] == \" Yes\", 1, 0)\n",
    "#     del kdd_census_df[\"sex\"]\n",
    "#     y = np.zeros(len(kdd_census_df))\n",
    "#     y[kdd_census_df[\"taxable income amount\"] == \" 50000+.\"] = 1\n",
    "#     kdd_census_df[\"income_binary\"] = y\n",
    "#     del kdd_census_df[\"taxable income amount\"]\n",
    "\n",
    "#     # print(kdd_census_df.head())\n",
    "#     # NOTE: e.g. fairness data survey recommends dropping some columns due to high missingness, e.g.:\n",
    "#     print(kdd_census_df[\"migration code-move within reg\"].unique())\n",
    "\n",
    "#     # drop missing values\n",
    "#     kdd_census_df.dropna(inplace=True, axis=1)\n",
    "#     print(kdd_census_df.isnull().sum())\n",
    "#     assert kdd_census_df.isnull().sum().sum() == 0, \"Encountered missing values!\"\n",
    "\n",
    "#     metadata_kdd = {\n",
    "#         \"name\": \"KDD-Census-Income\",\n",
    "#         \"protected_atts\": [\"sex_binary\"],\n",
    "#         \"protected_att_values\": [1],\n",
    "#         \"code\": [\"CE1\"],\n",
    "#         \"protected_att_descriptions\": [\"Gender = Female\"],\n",
    "#         \"target_variable\": \"income_binary\",\n",
    "#     }\n",
    "#     return kdd_census_df, feature_columns_kdd, metadata_kdd\n",
    "\n",
    "# from fl_puf.Utils.tabular_data_loader import dataset_to_numpy  \n",
    "\n",
    "# def get_tabular_numpy_dataset(dataset_name, num_sensitive_features, dataset_path=None):\n",
    "#     tmp = load_kdd(dataset_path=dataset_path)\n",
    "    \n",
    "#     _X, _Z, _y = dataset_to_numpy(*tmp, num_sensitive_features=num_sensitive_features)\n",
    "#     return _X, _Z, _y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, z, y = get_tabular_numpy_dataset(\n",
    "        dataset_name=\"kdd\",\n",
    "        num_sensitive_features=1,\n",
    "        dataset_path=f\"./data/Tabular/kdd/\",\n",
    "    )\n",
    "\n",
    "# X, z, y = load_kdd(dataset_path=f\"./data/Tabular/kdd/\")\n",
    "# z = z[:, 0]\n",
    "print(X.shape)\n",
    "\n",
    "# X, z, y = get_tabular_numpy_dataset(\n",
    "#         dataset_name=\"dutch\",\n",
    "#         num_sensitive_features=1,\n",
    "#         dataset_path=f\"./data/Tabular/dutch/\",\n",
    "#     )\n",
    "# z = z[:, 0]\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = []\n",
    "for target, sv in zip(y, z):\n",
    "    combinations.append((target.item(), sv.item()))\n",
    "\n",
    "print(Counter(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, z, y = get_tabular_numpy_dataset(\n",
    "#         dataset_name=\"adult\",\n",
    "#         num_sensitive_features=1,\n",
    "#         dataset_path=f\"./data/Tabular/adult/\",\n",
    "#     )\n",
    "# z = z[:, 0]\n",
    "# y = [int(item) for item in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = generate_clients_biased_data_mod(X, y, z, approach=\"representative\", num_nodes=150, ratio_unfair_nodes=0.5, opposite_direction=False, ratio_unfairness=(0, 0.3), group_to_reduce=(0,0), group_to_increment=(1,0), number_of_samples_per_node=300)\n",
    "disparities = compute_disparities_debug(nodes)\n",
    "# plot the bar plot of the disparities\n",
    "plot_bar_plot(title=\"Representative Diversity Approach\", disparities=disparities, nodes=[f\"{i}\" for i in range(len(nodes))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients_targets = []\n",
    "clients_sensitive_values = []\n",
    "clients_target_sensitive = []\n",
    "client_dataset_size = []\n",
    "\n",
    "# transform client data so that they are compatiblw with the\n",
    "# other functions\n",
    "tmp_data = []\n",
    "for client in nodes:\n",
    "    tmp_x = []\n",
    "    tmp_y = []\n",
    "    tmp_z = []\n",
    "    for sample in client:\n",
    "        tmp_x.append(sample[\"x\"])\n",
    "        tmp_y.append(sample[\"y\"])\n",
    "        tmp_z.append(sample[\"z\"])\n",
    "    tmp_data.append(\n",
    "        {\"x\": np.array(tmp_x), \"y\": np.array(tmp_y), \"z\": np.array(tmp_z)}\n",
    "    )\n",
    "client_data = tmp_data\n",
    "\n",
    "for client in client_data:\n",
    "    clients_targets.append(list(client[\"y\"]))\n",
    "    clients_sensitive_values.append(list(client[\"z\"]))\n",
    "    client_dataset_size.append(len(client[\"x\"]))\n",
    "    tmp = []\n",
    "    for target, sensitive_value in zip(list(client[\"y\"]), list(client[\"z\"])):\n",
    "        tmp.append((target, sensitive_value))   \n",
    "    clients_target_sensitive.append(tmp)\n",
    "\n",
    "counter_sensitive_features = [Counter(sensive_features) for sensive_features in clients_sensitive_values]\n",
    "\n",
    "counter_sensitive_features_0 = [counter[0.0] for counter in counter_sensitive_features]\n",
    "counter_sensitive_features_1 = [counter[1.0] for counter in counter_sensitive_features]\n",
    "\n",
    " # plot a barplot of the amount of sensitive feature 0 per client and the amount \n",
    " # of sensitive feature 1 per client in the same plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.bar(range(len(counter_sensitive_features_0)), counter_sensitive_features_0)\n",
    "\n",
    "plt.bar(range(len(counter_sensitive_features_1)), counter_sensitive_features_1, bottom=counter_sensitive_features_0)\n",
    "\n",
    "plt.xlabel(\"Client\")\n",
    "plt.ylabel(\"Amount of sensitive features\")\n",
    "plt.title(\"Amount of sensitive features per client\")\n",
    "plt.legend([\"Sensitive feature 0\", \"Sensitive feature 1\"])\n",
    "# font size 20\n",
    "plt.rcParams.update({'font.size': 20}) \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counter_groups = [Counter(client_data) for client_data in clients_target_sensitive]\n",
    "print(counter_groups[0])\n",
    "print(counter_groups[-1])\n",
    "counter_group_0_0 = [counter[(0, 0.0)] for counter in counter_groups]\n",
    "counter_group_0_1 = [counter[(0, 1.0)] for counter in counter_groups]\n",
    "counter_group_1_0 = [counter[(1, 0.0)] for counter in counter_groups]\n",
    "counter_group_1_1 = [counter[(1, 1.0)] for counter in counter_groups]\n",
    "\n",
    " # plot a barplot with counter_group_0_0, counter_group_0_1, counter_group_1_0, counter_group_1_1\n",
    " # for each client in the same plot\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.bar(range(len(counter_group_0_0)), counter_group_0_0)\n",
    "plt.bar(range(len(counter_group_0_1)), counter_group_0_1, bottom=counter_group_0_0)\n",
    "plt.bar(range(len(counter_group_1_0)), counter_group_1_0, bottom=[sum(x) for x in zip(counter_group_0_0, counter_group_0_1)])\n",
    "plt.bar(range(len(counter_group_1_1)), counter_group_1_1, bottom=[sum(x) for x in zip(counter_group_0_0, counter_group_0_1, counter_group_1_0)])\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Client\")\n",
    "plt.ylabel(\"Amount of samples\")\n",
    "plt.title(\"Samples for each group (target/sensitive Value) per client\")\n",
    "plt.legend([\"0,0\", \"0,1\", \"1,0\", \"1,1\"])\n",
    "# font size 20\n",
    "plt.rcParams.update({'font.size': 20}) \n",
    "plt.tight_layout()\n",
    "\n",
    "#group_to_reduce=(0,1), group_to_increment=(1,1),\n",
    "# 0, 0 -> 1, 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
