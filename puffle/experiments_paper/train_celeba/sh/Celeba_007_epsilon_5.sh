# # fixed 
# poetry run python ../../../main.py --batch_size=127 --clipping=9.41736245256686 --epochs=5 --lr=0.07586564140454176 --node_shuffle_seed=88097727 --optimizer=sgd --starting_lambda_value=0.689326854306812 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode fixed --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon 7.5 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_fixed
# poetry run python ../../../main.py --batch_size=127 --clipping=9.41736245256686 --epochs=5 --lr=0.07586564140454176 --node_shuffle_seed=88097728 --optimizer=sgd --starting_lambda_value=0.689326854306812 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode fixed --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon 7.5 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_fixed
# poetry run python ../../../main.py --batch_size=127 --clipping=9.41736245256686 --epochs=5 --lr=0.07586564140454176 --node_shuffle_seed=88097729 --optimizer=sgd --starting_lambda_value=0.689326854306812 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode fixed --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon 7.5 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_fixed
# poetry run python ../../../main.py --batch_size=127 --clipping=9.41736245256686 --epochs=5 --lr=0.07586564140454176 --node_shuffle_seed=88097720 --optimizer=sgd --starting_lambda_value=0.689326854306812 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode fixed --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon 7.5 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_fixed
# poetry run python ../../../main.py --batch_size=127 --clipping=9.41736245256686 --epochs=5 --lr=0.07586564140454176 --node_shuffle_seed=88097721 --optimizer=sgd --starting_lambda_value=0.689326854306812 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode fixed --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon 7.5 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_fixed

# # tunable 
# poetry run python ../../../main.py --alpha_target_lambda=1.8969331743229711 --batch_size=402 --clipping=18.348145534053653 --epochs=5 --lr=0.0846834884902614 --momentum=0.4881912190124535 --node_shuffle_seed=400763917 --optimizer=adam --weight_decay_lambda=0.3862308478627184 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode disparity --update_lambda True --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon_lambda 0.5 --epsilon 4 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_tunable
# poetry run python ../../../main.py --alpha_target_lambda=1.8969331743229711 --batch_size=402 --clipping=18.348145534053653 --epochs=5 --lr=0.0846834884902614 --momentum=0.4881912190124535 --node_shuffle_seed=400763918 --optimizer=adam --weight_decay_lambda=0.3862308478627184 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode disparity --update_lambda True --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon_lambda 0.5 --epsilon 4 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_tunable
# poetry run python ../../../main.py --alpha_target_lambda=1.8969331743229711 --batch_size=402 --clipping=18.348145534053653 --epochs=5 --lr=0.0846834884902614 --momentum=0.4881912190124535 --node_shuffle_seed=400763919 --optimizer=adam --weight_decay_lambda=0.3862308478627184 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode disparity --update_lambda True --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon_lambda 0.5 --epsilon 4 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_tunable
# poetry run python ../../../main.py --alpha_target_lambda=1.8969331743229711 --batch_size=402 --clipping=18.348145534053653 --epochs=5 --lr=0.0846834884902614 --momentum=0.4881912190124535 --node_shuffle_seed=400763910 --optimizer=adam --weight_decay_lambda=0.3862308478627184 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode disparity --update_lambda True --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon_lambda 0.5 --epsilon 4 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_tunable
# poetry run python ../../../main.py --alpha_target_lambda=1.8969331743229711 --batch_size=402 --clipping=18.348145534053653 --epochs=5 --lr=0.0846834884902614 --momentum=0.4881912190124535 --node_shuffle_seed=400763911 --optimizer=adam --weight_decay_lambda=0.3862308478627184 --dataset celeba --num_rounds 39 --num_client_cpus 1 --num_client_gpus 0.05 --pool_size 150 --sampled_clients 0.30 --sampled_clients_test 1.0 --train_csv original_merged --debug False --base_path ../../../../data --dataset_path ../../../../data/celeba --private True --DPL True --seed 41 --wandb True --training_nodes 0.67 --test_nodes 0.335 --starting_lambda_mode disparity --update_lambda True --target 0.075 --partition_type representative --group_to_reduce 1 1 --group_to_increment 0 1 --number_of_samples_per_node 1350 --ratio_unfair_nodes 0.5 --ratio_unfairness 0.9 0.9 --epsilon_lambda 0.5 --epsilon 4 --epsilon_statistics 0.5 --one_group_nodes True --project_name Celeba_Paper --run_name Celeba_007_epsilon_5_tunable